{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/airoprojects/visiope/blob/main/dataloader/load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_I9uEV0uwCV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle #used to save dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set up dataset directory path\n",
        "ckpt = Path('/content/drive/MyDrive/Dataset/')\n",
        "ckpt.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "xsEy-Hdr-2o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd41860d-9c1f-416d-f15b-068ed42506cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" if you already have the daset in your local drive, \n",
        "place it in a Dataset directory and unzip it from there \"\"\"\n",
        "\n",
        "# unzip dataset\n",
        "!unzip \"/content/drive/MyDrive/Dataset/ai4mars-dataset-merged-0.1.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "Rk1x19hGj87p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" else download und unzip the dataset from here \"\"\"\n",
        "\n",
        "import gdown\n",
        "\n",
        "# get url of zipped dataset\n",
        "url = 'https://drive.google.com/uc?id=1eW9Ah9DDEY02CTHCrRYLGPmiGZvCTKK4'\n",
        "\n",
        "# set up zip download location and start download\n",
        "output = '/content/ai4mars-dataset-merged-0.1.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# unzip dataset\n",
        "!unzip \"/content/ai4mars-dataset-merged-0.1.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "BE1VfEkQkAmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HjFSoKGUmVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Path\n",
        "images = \"ai4mars-dataset-merged-0.1/msl/images\"\n",
        "label_train = \"ai4mars-dataset-merged-0.1/msl/labels/train\"\n",
        "label_test_1ag = \"ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min1-100agree\"\n",
        "label_test_2ag = \"ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min2-100agree\"\n",
        "label_test_3ag = \"ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min3-100agree\"\n",
        "edr = images + \"/edr\"\n",
        "mxy = images + \"/mxy\" \n",
        "rng = images + \"/rng-30m\"\n",
        "\n",
        "\n",
        "edr_files = os.listdir(edr)\n",
        "label_train_files = os.listdir(label_train)\n",
        "label_test_files_1 = os.listdir(label_test_1ag)\n",
        "label_test_files_2 = os.listdir(label_test_2ag)\n",
        "label_test_files_3 = os.listdir(label_test_3ag)\n",
        "\n",
        "\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "y1 = []\n",
        "y2 = []\n",
        "y3 = []\n",
        "\n",
        "c = 0\n",
        "size = 224\n",
        "\n",
        "\n",
        "\n",
        "for label in label_test_files_1:\n",
        "    path_label = os.path.join(label_test_1ag, label)\n",
        "\n",
        "    img_arr = cv2.imread(path_label,0) #read image\n",
        "    img_arr = cv2.resize(img_arr, dsize=(size, size),interpolation=cv2.INTER_NEAREST) #resize\n",
        "    y1.append(img_arr)\n",
        "\n",
        "for label in label_test_files_2:\n",
        "    path_label = os.path.join(label_test_1ag, label)\n",
        "\n",
        "    img_arr = cv2.imread(path_label,0) #read image\n",
        "    img_arr = cv2.resize(img_arr, dsize=(size, size),interpolation=cv2.INTER_NEAREST) #resize\n",
        "    y2.append(img_arr)\n",
        "\n",
        "for label in label_test_files_3:\n",
        "    path_label = os.path.join(label_test_1ag, label)\n",
        "\n",
        "    img_arr = cv2.imread(path_label,0) #read image\n",
        "    img_arr = cv2.resize(img_arr, dsize=(size, size),interpolation=cv2.INTER_NEAREST) #resize\n",
        "    y3.append(img_arr)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "for label in label_train_files:\n",
        "\n",
        "\n",
        " \n",
        "    img_name = label[:-4] + \".JPG\" #Names of images match names of labels, except for the extension (JPG, png)\n",
        "\n",
        "    if img_name in edr_files:\n",
        "\n",
        "        img_path = os.path.join(edr, img_name) #get the full path\n",
        "        \n",
        "        # this is ugly, to change!\n",
        "        img_arr = cv2.imread(img_path) #read image\n",
        "        img_arr = cv2.resize(img_arr, dsize=(size, size)) #resize\n",
        "\n",
        "\n",
        "        label_path = os.path.join(label_train, label)\n",
        "        lab_arr = cv2.imread(label_path,0) #0 mean read as greyscale image\n",
        "        lab_arr = cv2.resize(lab_arr, (size,size), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "        X.append(img_arr)\n",
        "        print(label_path)\n",
        "        y.append(lab_arr)\n",
        "\n",
        "        #this control how much images you want\n",
        "        c+=1\n",
        "        if c==200: break\n",
        "\n",
        "\n",
        "X = np.asanyarray(X, dtype= np.float32) / 255\n",
        "y = np.array(y, dtype= np.int64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 0 - soil --> 0 \n",
        "# 1 - bedrock --> 1\n",
        "# 2 - sand --> 2\n",
        "# 3 - big rock --> 3\n",
        "# 255 -> 4 - NULL (no label)\n",
        "\n",
        "\n",
        "y[y==255] = 4\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(y[1])\n",
        "\n",
        "\n",
        "#from numpy array to torch\n",
        "\n",
        "Xt = torch.from_numpy(X)\n",
        "yt = torch.from_numpy(y)\n",
        "\n",
        "print(Xt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "plt.imshow(X[0])\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "validation set parametrized to decide if you want only 1, 2, oe togheter\n",
        "parametrized \n",
        "'''\n"
      ],
      "metadata": {
        "id": "aX9Zlp8HM9wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This class rappresents the dataset \n",
        "class Ai4MarsData(Dataset):\n",
        "    #X tensor (torch) -> images\n",
        "    #y tensor (torch) -> labels\n",
        "\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.X[index]\n",
        "        label = self.y[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "    \n",
        "\n",
        "#create a dataset\n",
        "dataset = Ai4MarsData(Xt,yt)\n",
        "#create a dataloader\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size,shuffle=False)\n",
        "\n",
        "# Define a dictionary that contains both the DataLoader object and the class definition\n",
        "data_dict = {\"dataloader\": data_loader, \"class\":Ai4MarsData}\n",
        "\n",
        "# Save the data loader as a pickle file\n",
        "with open('/content/drive/MyDrive/Dataset/data_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(data_dict, f)"
      ],
      "metadata": {
        "id": "otUTgv6v8Is7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiTAPAa9elhO"
      },
      "outputs": [],
      "source": [
        "# General imports\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics as metrics\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ywxl7By7Pv3"
      },
      "outputs": [],
      "source": [
        "# Custom Imports\n",
        "\n",
        "COLAB = 'google.colab' in sys.modules\n",
        "LOCAL = not COLAB\n",
        "\n",
        "if COLAB:\n",
        "\n",
        "    # Clone visiope repo on runtime env\n",
        "    !git clone https://github.com/airoprojects/visiope.git /content/visiope/\n",
        "    %cd ./visiope/\n",
        "    !git checkout tests\n",
        "    !git branch\n",
        "    %cd ../\n",
        "\n",
        "    # Get the root directory of the Git project\n",
        "    root_dir = '/content/visiope'\n",
        "\n",
        "    # Add custom modules to path\n",
        "    custom_modules_path = root_dir + '/tools/'\n",
        "    sys.path.insert(0, custom_modules_path)\n",
        "\n",
        "elif LOCAL:\n",
        "\n",
        "    from git import Repo\n",
        "\n",
        "    # Initialize the Git repository object\n",
        "    repo = Repo(\".\", search_parent_directories=True)\n",
        "\n",
        "    # Get the root directory of the Git project\n",
        "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
        "\n",
        "    # Add custom modules to path\n",
        "    custom_modules_path = root_dir  + '/tools/'\n",
        "    sys.path.insert(0, custom_modules_path)\n",
        "\n",
        "\n",
        "# Import Loader\n",
        "from data.utils import Ai4MarsDownload, Ai4MarsSplitter, Ai4MarsDataLoader\n",
        "\n",
        "# Import Loss\n",
        "from loss.loss import Ai4MarsCrossEntropy, Ai4MarsDiceLoss\n",
        "\n",
        "# Import Trainer\n",
        "from trainer.trainer import Ai4MarsTrainer\n",
        "\n",
        "# Import Tester\n",
        "from tester.tester import Ai4MarsTester\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmRVji71wGJc"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "\n",
        "# Set this to True if you wnat to load directly the dataloader\n",
        "# this can be done only on colab and it is useful to avoid runtime crash\n",
        "LOAD = False\n",
        "\n",
        "if LOAD:\n",
        "\n",
        "    if COLAB:\n",
        "\n",
        "        if not(os.path.exists('/content/dataset/')):\n",
        "\n",
        "            import gdown\n",
        "\n",
        "            # get url of torch dataset (temporarerly my drive)\n",
        "            drive = 'https://drive.google.com/uc?id='\n",
        "            url = 'https://drive.google.com/drive/folders/104YvO3LcU76euuVe-_62eS_Rld-tOZeh?usp=drive_link'\n",
        "\n",
        "            !gdown --folder {url} -O /content/\n",
        "\n",
        "            load_path = '/content/dataset/dataset.pt'\n",
        "\n",
        "    elif LOCAL: load_path = root_dir + '/datasetup/dataset/dataset.pt'\n",
        "\n",
        "    dataset = torch.load(load_path)\n",
        "    train_set = dataset[0]\n",
        "    test_set = dataset[1]\n",
        "    val_set = dataset[2]\n",
        "\n",
        "    # Build Ai4MarsDataloader\n",
        "    loader = Ai4MarsDataLoader()\n",
        "    train_loader, test_loader, val_loader = loader(\n",
        "        [train_set, test_set, val_set], [32, 16, 16])\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "    # Insert here your local path to the dataset (temporary)\n",
        "    data_path ='/home/leeoos/Desktop/' #input(\"Path to Dataset: \")\n",
        "\n",
        "    # Insert here the number of images you want to download\n",
        "    num_images = 500 #int(input(\"Number of images (max 1000): \"))\n",
        "\n",
        "    save_path = None\n",
        "    # Uncomment the following line to the dataset on a local path\n",
        "    #save_path = root_dir + '/datasetup/dataset/'\n",
        "\n",
        "    if num_images > 1000 : raise Exception(\"Trying to import too many images\")\n",
        "\n",
        "    # Import data as Ai4MarsDataset\n",
        "    importer = Ai4MarsDownload()\n",
        "    X, y = importer(PATH=data_path, NUM_IMAGES=num_images)\n",
        "\n",
        "    transform = None\n",
        "    # Uncomment the following lines to apply transformations to the dataset\n",
        "    '''\n",
        "    transform = transforms.RandomChoice([\n",
        "     transforms.RandomRotation(90)])\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # Split the dataset\n",
        "    splitter = Ai4MarsSplitter()\n",
        "    train_set, test_set, val_set = splitter(X, y, [0.7, 0.2, 0.1], transform=transform,\n",
        "                                            SAVE_PATH=save_path, SIZE=128)\n",
        "\n",
        "    # Build Ai4MarsDataloader\n",
        "    loader = Ai4MarsDataLoader()\n",
        "    train_loader, test_loader, val_loader = loader([train_set, test_set, val_set], [32, 16, 16],\n",
        "                                                   SIZE=128, SAVE_PATH=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "print(len(val_set))\n",
        "\n",
        "image, label = train_set.__getitem__(0)\n",
        "\n",
        "print(f'image shape: {image.permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(image.permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(f'image shape: {label.permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(label.permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "k = 0 \n",
        "for image, label in train_loader:\n",
        "    if k == 1: break\n",
        "    k += 1\n",
        "\n",
        "print(f'image shape: {image[0].permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(image[0].permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(f'image shape: {label[0].permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(label[0].permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FCQvjMbelhL"
      },
      "outputs": [],
      "source": [
        "# Clone remote repo with existing models\n",
        "\n",
        "if COLAB:\n",
        "    !git clone https://github.com/sithu31296/semantic-segmentation\n",
        "    %cd semantic-segmentation\n",
        "    %pip install -e .\n",
        "    %pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCa6tmRlelhW"
      },
      "outputs": [],
      "source": [
        "#  Import segformer\n",
        "\n",
        "from semseg.models import *\n",
        "\n",
        "model = eval('SegFormer')(\n",
        "    backbone='MiT-B1',\n",
        "    num_classes=5\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth',\n",
        "                                     map_location=device))\n",
        "    print(\"Pretrained model's weights downloaded\")\n",
        "except:\n",
        "    print(\"Download a pretrained model's weights from the result table.\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print('Loaded Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSxW3mFNelhl"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "\n",
        "loss_fn = Ai4MarsDiceLoss().to(device)\n",
        "#loss_fn = Ai4MarsCrossEntropy().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "transform = transforms.RandomChoice([\n",
        "     transforms.RandomRotation(90)])\n",
        "\n",
        "trainer = Ai4MarsTrainer(loss_fn, optimizer, train_loader, val_loader, transform=transform, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6p6LYs2R0fF"
      },
      "outputs": [],
      "source": [
        "# Module Parameters\n",
        "trainer.param_hist(model, SAVE_PATH=root_dir+'/experiments', label='before')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSkhk_Rl2gX0"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train_multiple_epoch(model, EPOCHS=5, SAVE_PATH=root_dir+'/experiments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6HCej-k7w6R"
      },
      "outputs": [],
      "source": [
        "# Plot loss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainer.plot_loss(model=model, SAVE_PATH=root_dir+'/experiments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(train_loader))\n",
        "print(len(train_loader.dataset))\n",
        "print(train_loader.batch_size)\n",
        "import math\n",
        "print(math.ceil(len(train_loader.dataset) / train_loader.batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydweI29r18jr"
      },
      "outputs": [],
      "source": [
        "# Testing and evaluation Metrics\n",
        "\n",
        "metric = metrics.JaccardIndex(task=\"multiclass\", num_classes=5).to(device)\n",
        "tester = Ai4MarsTester(loss_fn, metric, test_loader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZyzuJW61Rs"
      },
      "outputs": [],
      "source": [
        "# Start testing\n",
        "\n",
        "tester.test_one_epoch(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1_v2Q1AV73W"
      },
      "outputs": [],
      "source": [
        "# Module Parameters\n",
        "trainer.param_hist(model, SAVE_PATH=root_dir+'/experiments', label='after')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

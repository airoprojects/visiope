{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/airoprojects/visiope/blob/main/experiments/test_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA344_H5pmEP"
      },
      "source": [
        "## General imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiTAPAa9elhO"
      },
      "outputs": [],
      "source": [
        "# General imports\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics as metrics\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lygw33bMprOM"
      },
      "source": [
        "## Custom Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ywxl7By7Pv3"
      },
      "outputs": [],
      "source": [
        "# Custom Imports\n",
        "import sys\n",
        "\n",
        "COLAB = 'google.colab' in sys.modules\n",
        "LOCAL = not COLAB\n",
        "\n",
        "if COLAB:\n",
        "\n",
        "    # Clone visiope repo on runtime env\n",
        "    !git clone https://github.com/airoprojects/visiope.git /content/visiope/\n",
        "    %cd ./visiope/\n",
        "    !git checkout tests\n",
        "    !git branch\n",
        "    %cd ../\n",
        "\n",
        "    # Get the root directory of the Git project\n",
        "    root_dir = '/content/visiope'\n",
        "\n",
        "    # Add custom modules to path\n",
        "    custom_modules_path = root_dir + '/tools/'\n",
        "    sys.path.insert(0, custom_modules_path)\n",
        "\n",
        "elif LOCAL:\n",
        "\n",
        "    from git import Repo\n",
        "\n",
        "    # Initialize the Git repository object\n",
        "    repo = Repo(\".\", search_parent_directories=True)\n",
        "\n",
        "    # Get the root directory of the Git project\n",
        "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
        "\n",
        "    # Add custom modules to path\n",
        "    custom_modules_path = root_dir  + '/tools/'\n",
        "    sys.path.insert(0, custom_modules_path)\n",
        "\n",
        "\n",
        "# Import Loader\n",
        "from data.utils import Ai4MarsDownload, Ai4MarsImporter, Ai4MarsSplitter, Ai4MarsDataLoader\n",
        "\n",
        "# Import Loss\n",
        "from loss.loss import Ai4MarsCrossEntropy, Ai4MarsDiceLoss\n",
        "\n",
        "# Import Trainer\n",
        "from trainer.trainer import Ai4MarsTrainer\n",
        "\n",
        "# Import Tester\n",
        "from tester.tester import Ai4MarsTester\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRMqpowGpvoQ"
      },
      "source": [
        "## Obtain dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qe47EV7tZ_R"
      },
      "source": [
        "#### Selecter functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTO2wZubthnU"
      },
      "outputs": [],
      "source": [
        "# get the whole dataset\n",
        "def get_d(b):\n",
        "\n",
        "    global num_images\n",
        "    global data_path\n",
        "    global selector\n",
        "    global X\n",
        "    global y\n",
        "\n",
        "    # Cannot select anymore\n",
        "    if not selector:\n",
        "        print(\"Cannot select multiple options\")\n",
        "        return\n",
        "    selector = False\n",
        "\n",
        "    # Insert here your local path to the dataset (temporary)\n",
        "    data_path = input(\"Path to Dataset: \")\n",
        "\n",
        "    # Insert here the number of images you want to download\n",
        "    num_images = int(input(\"Number of images (max 1000): \"))\n",
        "\n",
        "    # Import data as Ai4MarsDataset\n",
        "    Ai4MarsDownload()(PATH=data_path)\n",
        "    importer = Ai4MarsImporter()\n",
        "    X, y, _ = importer(PATH=data_path, NUM_IMAGES=num_images, SIZE=128)\n",
        "\n",
        "    pass\n",
        "\n",
        "# Enable loading chunk of data\n",
        "def enable_chunk(b):\n",
        "\n",
        "    global selector\n",
        "    global CHUNK\n",
        "    global current_chunk\n",
        "\n",
        "    # Cannot select anymore\n",
        "    if not selector:\n",
        "        print(\"Cannot select multiple options\")\n",
        "        return\n",
        "    selector = False\n",
        "\n",
        "    CHUNK = True\n",
        "    current_chunk = 0\n",
        "\n",
        "    # Download options\n",
        "    d300 = widgets.Button(description=\"Dataset 300\")\n",
        "    d500 = widgets.Button(description=\"Dataset 500\")\n",
        "    d1k = widgets.Button(description=\"Dataset 1000\")\n",
        "\n",
        "    # Actions\n",
        "    display(d300, d500, d1k)\n",
        "    d300.on_click(get_d300)\n",
        "    d500.on_click(get_d500)\n",
        "    d1k.on_click(get_d1k)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cmnA4w0Gv-m"
      },
      "source": [
        "#### Data download funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp_QfZkaG1XJ"
      },
      "outputs": [],
      "source": [
        "# ge the dataset in chncks of size 300\n",
        "def get_d300(b):\n",
        "\n",
        "    global COLAB\n",
        "    global CHUNK\n",
        "    global current_chunk\n",
        "    global chunk_size\n",
        "    global total_size\n",
        "    global X\n",
        "    global y\n",
        "\n",
        "    # Cannot select anymore\n",
        "    if not CHUNK:\n",
        "        print(\"Cannot select multiple options\")\n",
        "        return\n",
        "\n",
        "    chunk_size = 300\n",
        "    total_size = 3000\n",
        "\n",
        "    # update current chunk\n",
        "    current_chunk += chunk_size\n",
        "\n",
        "    print(f\"Loading cunck {current_chunk // chunk_size}\")\n",
        "\n",
        "    if COLAB:\n",
        "\n",
        "        if not(os.path.exists('/content/dataset/')):\n",
        "\n",
        "            import gdown\n",
        "\n",
        "            # get url of torch dataset (temporarerly my drive)\n",
        "            drive = 'https://drive.google.com/uc?id='\n",
        "            url = 'https://drive.google.com/drive/folders/1WaQKmnty99798t6b-GMh15GVy9ZxCkGP?usp=drive_link'\n",
        "\n",
        "            !gdown --folder {url} -O /content/\n",
        "\n",
        "            load_data = '/content/small-chunk-dataset/'\n",
        "\n",
        "    elif LOCAL:\n",
        "        load_data = root_dir + '/datasetup/small-chunk-dataset/'\n",
        "\n",
        "    X, y = torch.load(load_data + str(current_chunk // chunk_size) + 'dataset.pt')\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "    pass\n",
        "\n",
        "# ge the dataset in chncks of size 500\n",
        "def get_d500(b):\n",
        "\n",
        "    global COLAB\n",
        "    global CHUNK\n",
        "    global current_chunk\n",
        "    global chunk_size\n",
        "    global total_size\n",
        "    global X\n",
        "    global y\n",
        "\n",
        "    # Cannot select anymore\n",
        "    if not CHUNK:\n",
        "        print(\"Cannot select multiple options\")\n",
        "        return\n",
        "\n",
        "    chunk_size = 500\n",
        "    total_size = 5000\n",
        "\n",
        "    # update current chunk\n",
        "    current_chunk += chunk_size\n",
        "\n",
        "    print(f\"Loading cunck {current_chunk // chunk_size}\")\n",
        "\n",
        "    if COLAB:\n",
        "\n",
        "        if not(os.path.exists('/content/dataset/')):\n",
        "\n",
        "            import gdown\n",
        "\n",
        "            # get url of torch dataset (temporarerly my drive)\n",
        "            drive = 'https://drive.google.com/uc?id='\n",
        "            url = 'https://drive.google.com/drive/folders/1u4imSO8MerdZEW0V1lkEb6PWQmZgT5Ft?usp=sharing'\n",
        "\n",
        "            !gdown --folder {url} -O /content/\n",
        "\n",
        "            load_data = '/content/medium-chunk-dataset/'\n",
        "\n",
        "    elif LOCAL:\n",
        "        load_data = root_dir + '/datasetup/medium-chunk-dataset/'\n",
        "\n",
        "    X, y = torch.load(load_data + str(current_chunk // chunk_size) + 'dataset.pt')\n",
        "\n",
        "    print(\"Done\")\n",
        "    pass\n",
        "\n",
        "# ge the dataset in chncks of size 1k\n",
        "def get_d1k(b):\n",
        "\n",
        "    global COLAB\n",
        "    global CHUNK\n",
        "    global current_chunk\n",
        "    global chunk_size\n",
        "    global total_size\n",
        "    global X\n",
        "    global y\n",
        "\n",
        "    # Cannot select anymore\n",
        "    if not CHUNK:\n",
        "        print(\"Cannot select multiple options\")\n",
        "        return\n",
        "\n",
        "    chunk_size = 1000\n",
        "    total_size = 10000\n",
        "\n",
        "    print(f\"Loading cunck {current_chunk // chunk_size}\")\n",
        "\n",
        "    # update current chunk\n",
        "    current_chunk += chunk_size\n",
        "\n",
        "    if COLAB:\n",
        "\n",
        "        if not(os.path.exists('/content/dataset/')):\n",
        "\n",
        "            import gdown\n",
        "\n",
        "            # get url of torch dataset (temporarerly my drive)\n",
        "            drive = 'https://drive.google.com/uc?id='\n",
        "            url = 'https://drive.google.com/drive/folders/1BhL0nLbQt930l_ISDgUS8ZJRWGjH4eUU?usp=sharing'\n",
        "\n",
        "            !gdown --folder {url} -O /content/\n",
        "\n",
        "            load_data = '/content/chunk-dataset/'\n",
        "\n",
        "    elif LOCAL:\n",
        "        load_data = root_dir + '/datasetup/chunk-dataset/'\n",
        "\n",
        "    X, y = torch.load(load_data + 'dataset_' + str(current_chunk) + '.pt')\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR-6RoS3t1IX"
      },
      "source": [
        "#### Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6wXNW5yp4Zh"
      },
      "outputs": [],
      "source": [
        "# Select import mode\n",
        "\n",
        "# ENV VARIABLE INITIALIZATION DO NOT CHANGE\n",
        "CHUNK = False\n",
        "current_chunk = 0\n",
        "chunk_size = 0\n",
        "total_size = 0\n",
        "X = 0\n",
        "y = 0\n",
        "\n",
        "# Allow to select only one option\n",
        "if 'selector' not in globals():\n",
        "    selector  = True\n",
        "\n",
        "# Selection buttons\n",
        "d = widgets.Button(description=\"Load whole Dataset\")\n",
        "chunk = widgets.Button(description=\"Load chunk of data\")\n",
        "\n",
        "\n",
        "# actions\n",
        "display(d, chunk)\n",
        "d.on_click(get_d)\n",
        "chunk.on_click(enable_chunk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXVyvO5aeyR0"
      },
      "outputs": [],
      "source": [
        "# HARD RESET\n",
        "# selector = True\n",
        "# CHUNK = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0827HU01q6nH"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca-BSlVTq_Ew"
      },
      "outputs": [],
      "source": [
        "transform = None\n",
        "# Uncomment the following lines to apply transformations to the dataset\n",
        "'''\n",
        "transform = transforms.RandomChoice([\n",
        "    transforms.RandomRotation(90)])\n",
        "'''\n",
        "\n",
        "# Split the dataset\n",
        "splitter = Ai4MarsSplitter()\n",
        "percentages = [0.7, 0.2, 0.1]\n",
        "train_set, test_set, val_set = splitter(X, y, percentages, transform=transform)\n",
        "\n",
        "# Load info\n",
        "load_info = './.info.pt'\n",
        "info = torch.load(load_info)\n",
        "\n",
        "# Build Ai4MarsDataloader\n",
        "loader = Ai4MarsDataLoader()\n",
        "batch_sizes = [32, 16, 16]\n",
        "datasets = [train_set, test_set, val_set]\n",
        "train_loader, test_loader, val_loader = loader(datasets, batch_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpXj7Ow-wrL5"
      },
      "outputs": [],
      "source": [
        "# Just to check that different chunks contains different images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f'Len of train set: {len(train_set)}')\n",
        "print(f'Len of test set: {len(test_set)}')\n",
        "print(f'Len of validation set: {len(val_set)}')\n",
        "\n",
        "image, label = X[0], y[0]\n",
        "\n",
        "print(f'image shape: {image.permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(image.permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(f'image shape: {label.permute(1,0,2).permute(0,2,1).shape}')\n",
        "plt.imshow(label.permute(1,0,2).permute(0,2,1).detach().numpy(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Free up mem\n",
        "del X\n",
        "del y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYPFyV05rDWE"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FCQvjMbelhL"
      },
      "outputs": [],
      "source": [
        "# Clone remote repo with existing models\n",
        "\n",
        "if COLAB:\n",
        "    !git clone https://github.com/sithu31296/semantic-segmentation\n",
        "    %cd semantic-segmentation\n",
        "    %pip install -e .\n",
        "    %pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf3AWmRO_2CJ"
      },
      "outputs": [],
      "source": [
        "from semseg import show_models\n",
        "\n",
        "show_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCa6tmRlelhW"
      },
      "outputs": [],
      "source": [
        "#  Import segformer\n",
        "\n",
        "from semseg.models import *\n",
        "\n",
        "model = eval('SegFormer')(\n",
        "    backbone='MiT-B1',\n",
        "    num_classes=5\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth',\n",
        "                                     map_location=device))\n",
        "    print(\"Pretrained model's weights downloaded\")\n",
        "except:\n",
        "    print(\"Download a pretrained model's weights from the result table.\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print('Loaded Model')\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTK2tqpgrILQ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi6ZGOtk3EXa"
      },
      "source": [
        "#### Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSxW3mFNelhl"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "\n",
        "loss_fn = Ai4MarsDiceLoss().to(device)\n",
        "#loss_fn = Ai4MarsCrossEntropy().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "transform = None\n",
        "# transform = transforms.RandomChoice([\n",
        "#      transforms.RandomRotation(90)])\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "trainer = Ai4MarsTrainer(loss_fn, optimizer, train_loader, val_loader,\n",
        "                         transform=transform, device=device, info=info, model_name='MiT-B1', dump=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6p6LYs2R0fF"
      },
      "outputs": [],
      "source": [
        "# Model Parameters\n",
        "# Check if the variable has been defined\n",
        "if 'run_once' not in globals():\n",
        "    run_once = True\n",
        "    trainer.param_hist(model, label='before')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZDmdPHeyR4"
      },
      "source": [
        "#### Regular training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSkhk_Rl2gX0"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train_multiple_epoch(model, EPOCHS=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U92PUhBW2-lv"
      },
      "source": [
        "#### Chunk training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6LW1nfz3MpV"
      },
      "outputs": [],
      "source": [
        "if not CHUNK:\n",
        "    print(\"You are not allowd to perform chunk training proceede with regular traiing\")\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"You are allowd to perform chunk training proceede with regular traiing\")\n",
        "    print(\"Please pay attention that you are using the same splitting parameters\")\n",
        "    CHUNK = False\n",
        "\n",
        "    for i in range(0, total_size, chunk_size):\n",
        "\n",
        "        if i == 0 :\n",
        "            trainer.train_multiple_epoch(model, EPOCHS=epochs)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # update current chunk\n",
        "            current_chunk += chunk_size\n",
        "\n",
        "            print(f\"Loading cunck {current_chunk // chunk_size}\")\n",
        "\n",
        "            if COLAB: load_data = '/content/chunk-dataset/'\n",
        "            elif LOCAL: load_data = root_dir + '/datasetup/dataset/'\n",
        "\n",
        "            X, y = torch.load(load_data + 'dataset_' + str(current_chunk) + '.pt')\n",
        "\n",
        "            # Build dataset\n",
        "            splitter = Ai4MarsSplitter()\n",
        "            train_set, test_set, val_set = splitter(X, y, percentages)\n",
        "\n",
        "            # Build Ai4MarsDataloader\n",
        "            loader = Ai4MarsDataLoader()\n",
        "            train_loader, test_loader, val_loader = loader(\n",
        "                [train_set, test_set, val_set], batch_sizes)\n",
        "\n",
        "            print(info)\n",
        "            trainer.train_multiple_epoch(model, EPOCHS=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BFxx3vyeyR5"
      },
      "source": [
        "#### Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6HCej-k7w6R"
      },
      "outputs": [],
      "source": [
        "# Plot loss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainer.plot_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQYontwr3W8"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydweI29r18jr"
      },
      "outputs": [],
      "source": [
        "# Testing and evaluation Metrics\n",
        "\n",
        "metric = metrics.JaccardIndex(task=\"multiclass\", num_classes=5).to(device)\n",
        "tester = Ai4MarsTester(loss_fn, metric, test_loader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZyzuJW61Rs"
      },
      "outputs": [],
      "source": [
        "# Start testing\n",
        "\n",
        "tester.test_one_epoch(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1_v2Q1AV73W"
      },
      "outputs": [],
      "source": [
        "# Module Parameters\n",
        "trainer.param_hist(model, label='after')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewE16aoK_2CX"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "for i, batch in enumerate(test_loader):\n",
        "  image, label = batch\n",
        "  print(image.shape)\n",
        "  pred = model(image.to(device))\n",
        "  tester.show_images(image, trainer.results_path, index=i)\n",
        "  tester.show_seg(label, trainer.results_path, index=i)\n",
        "  tester.show_seg(pred.cpu(), trainer.results_path, index=i)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3551,
     "status": "ok",
     "timestamp": 1689164151821,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "kytW-DGR_xaS",
    "outputId": "e2d0b3eb-5cdd-45d6-af7e-00fb52eb82f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIN_COLAB = \\'google.colab\\' in sys.modules\\n\\nif not IN_COLAB:\\n    from git import Repo\\n\\n    # Initialize the Git repository object\\n    repo = Repo(\".\", search_parent_directories=True)\\n\\n    # Get the root directory of the Git project\\n    root_dir = repo.git.rev_parse(\"--show-toplevel\")\\n\\n    from pathlib import Path\\n\\n    # Set up path for custom importer modules\\n    # Data Loader\\n    importer_module = root_dir + \\'/dataloader/\\'\\n    print(importer_module)\\n    sys.path.insert(0, importer_module)\\n    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\\n\\n    # Loss\\n\\n    loss_module = root_dir + \\'/trainer/loss/\\'\\n    sys.path.insert(0, loss_module)\\n    import loss\\n\\n    # Trainer\\n    trainer_module = root_dir + \\'/trainer/\\'\\n    sys.path.insert(0, trainer_module)\\n    from trainer import Ai4MarsTrainer\\n\\n    # Insert here your local path to the dataset (temporary)\\n    data_path = input(\"Path to Dataset: \") #\\'/home/leeoos/Desktop/\\'\\n\\nelse: # IN_COLAB\\n\\n    from google.colab import drive\\n    drive.mount(\\'/content/drive\\')\\n\\n    # On Colab the path to the module ti fixed once you have\\n    # corretly set up the project with gitsetup.ipynb\\n\\n    # Import Loader\\n    fixed_path_loader = \\'/content/drive/MyDrive/Github/visiope/dataloader/\\'\\n    sys.path.insert(0, fixed_path_loader)\\n    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\\n\\n    # Import Trainer\\n    fixed_path_trainer = \\'/content/drive/MyDrive/Github/visiope/trainer/\\'\\n    sys.path.insert(0,fixed_path_trainer )\\n    from trainer import Ai4MarsTrainer\\n\\n    # Import Loss\\n    fixed_path_loss = \\'/content/drive/MyDrive/Github/visiope/trainer/loss/\\'\\n    sys.path.insert(0, fixed_path_loss)\\n    import loss\\n    \\n    !git clone https://github.com/sithu31296/semantic-segmentation\\n    %pip install -U gdown\\n    %pip install -e .\\n    %pip install einops\\n    import gdown\\n    from pathlib import Path\\n\\n    ckpt = Path(\\'./checkpoints/pretrained/segformer\\')\\n    ckpt.mkdir(exist_ok=True, parents=True)\\n\\n    url = \\'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT\\'\\n    output = \\'./checkpoints/pretrained/segformer/segformer.b3.ade.pth\\'\\n\\n    gdown.download(url, output, quiet=False)\\n\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n%cd semantic-segmentation\\n#/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/dataloader/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import io\n",
    "from torchvision import transforms \n",
    "from PIL import Image\n",
    "import pickle\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import gc\n",
    "from typing import Tuple\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "'''\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if not IN_COLAB:\n",
    "    from git import Repo\n",
    "\n",
    "    # Initialize the Git repository object\n",
    "    repo = Repo(\".\", search_parent_directories=True)\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Set up path for custom importer modules\n",
    "    # Data Loader\n",
    "    importer_module = root_dir + '/dataloader/'\n",
    "    print(importer_module)\n",
    "    sys.path.insert(0, importer_module)\n",
    "    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n",
    "\n",
    "    # Loss\n",
    "\n",
    "    loss_module = root_dir + '/trainer/loss/'\n",
    "    sys.path.insert(0, loss_module)\n",
    "    import loss\n",
    "\n",
    "    # Trainer\n",
    "    trainer_module = root_dir + '/trainer/'\n",
    "    sys.path.insert(0, trainer_module)\n",
    "    from trainer import Ai4MarsTrainer\n",
    "\n",
    "    # Insert here your local path to the dataset (temporary)\n",
    "    data_path = input(\"Path to Dataset: \") #'/home/leeoos/Desktop/'\n",
    "\n",
    "else: # IN_COLAB\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # On Colab the path to the module ti fixed once you have\n",
    "    # corretly set up the project with gitsetup.ipynb\n",
    "\n",
    "    # Import Loader\n",
    "    fixed_path_loader = '/content/drive/MyDrive/Github/visiope/dataloader/'\n",
    "    sys.path.insert(0, fixed_path_loader)\n",
    "    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n",
    "\n",
    "    # Import Trainer\n",
    "    fixed_path_trainer = '/content/drive/MyDrive/Github/visiope/trainer/'\n",
    "    sys.path.insert(0,fixed_path_trainer )\n",
    "    from trainer import Ai4MarsTrainer\n",
    "\n",
    "    # Import Loss\n",
    "    fixed_path_loss = '/content/drive/MyDrive/Github/visiope/trainer/loss/'\n",
    "    sys.path.insert(0, fixed_path_loss)\n",
    "    import loss\n",
    "    \n",
    "    !git clone https://github.com/sithu31296/semantic-segmentation\n",
    "    %pip install -U gdown\n",
    "    %pip install -e .\n",
    "    %pip install einops\n",
    "    import gdown\n",
    "    from pathlib import Path\n",
    "\n",
    "    ckpt = Path('./checkpoints/pretrained/segformer')\n",
    "    ckpt.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n",
    "    output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n",
    "\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%cd semantic-segmentation\n",
    "#/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/dataloader/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/models/semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "# Custom Imports\n",
    "\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "LOCAL = not COLAB\n",
    "\n",
    "if COLAB:\n",
    "\n",
    "    # Clone visiope repo on runtime env\n",
    "    !git clone https://github.com/airoprojects/visiope.git /\n",
    "\n",
    "    # Install pytorchmetrics\n",
    "    !pip install torchmetrics\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = '/content/visiope'\n",
    "\n",
    "    # Add custom modules to path\n",
    "    custom_modules_path = root_dir + '/tools/'\n",
    "    sys.path.insert(0, custom_modules_path)\n",
    "\n",
    "elif LOCAL:\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "    from git import Repo\n",
    "\n",
    "    # Initialize the Git repository object\n",
    "    repo = Repo(\".\", search_parent_directories=True)\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "    # Add custom modules to path\n",
    "    custom_modules_path = root_dir  + '/tools/'\n",
    "    sys.path.insert(0, custom_modules_path)\n",
    "\n",
    "\n",
    "# Import Loader\n",
    "from data.utils import Ai4MarsDownload, Ai4MarsSplitter, Ai4MarsDataLoader\n",
    "\n",
    "# Import Loss\n",
    "from loss.loss import Ai4MarsCrossEntropy, Ai4MarsDiceLoss\n",
    "\n",
    "# Import Trainer\n",
    "from trainer.trainer import Ai4MarsTrainer\n",
    "\n",
    "# Import Tester\n",
    "from tester.tester import Ai4MarsTester\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%cd semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1689164151824,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "kwVFZJKgXULT"
   },
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, nc):\n",
    "            self.nc = nc\n",
    "            super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "        def forward(self, embeds):\n",
    "            ngf = 64\n",
    "            nz = [[None] *6]*4\n",
    "\n",
    "            j = 0\n",
    "            for embed in embeds:\n",
    "                nz[j][0] = embed.size()[1]\n",
    "                for i in range(1,6):\n",
    "                    if i < 4:\n",
    "                        nz[j][i] = nz[i-1]//2\n",
    "                    else:\n",
    "                        nz[j][i] = nz[i-1]//4\n",
    "                j = j+1\n",
    "                print(nz)\n",
    "            self.main = nn.Sequential(\n",
    "                #reduction of dimensionality To Be Changed in conv... maybe\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d( nz[6], ngf * 8, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 8),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*8) x 4 x 4``\n",
    "                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*4) x 8 x 8``\n",
    "                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*2) x 16 x 16``\n",
    "                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf) x 32 x 32``\n",
    "                nn.ConvTranspose2d( ngf, self.nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh()\n",
    "                # state size. ``(nc) x 64 x 64``\n",
    "            )\n",
    "\n",
    "            return self.main(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1689164151825,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "1n9BBXOOn-Cl"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super(Generator, self).__init__()\n",
    "        ngf = 64\n",
    "        nc = 1\n",
    "        print(nz)\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1689164151827,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "ZD2IYalLn-Co"
   },
   "outputs": [],
   "source": [
    "test1 = True\n",
    "if test1:\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, nz):\n",
    "            super(Generator, self).__init__()\n",
    "            ngf = 64\n",
    "            nc = 5\n",
    "            self.main = nn.Sequential()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                #in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 8),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*8) x 4 x 4``\n",
    "                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*4) x 8 x 8``\n",
    "                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*2) x 16 x 16``\n",
    "                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf) x 32 x 32``\n",
    "                nn.ConvTranspose2d( ngf, ngf//2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf//2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf/2) x 64 x 64``\n",
    "                nn.ConvTranspose2d( ngf//2, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh()\n",
    "                # state size. ``(ngc) x 128 x 128``\n",
    "                # state size. ``(ngf/4) x 128 x 128``\n",
    "                #nn.BatchNorm2d(ngf),\n",
    "                #nn.ReLU(True),\n",
    "                #nn.ConvTranspose2d( ngf/4, nc, 4, 2, 1, bias=False),\n",
    "                #nn.Tanh()\n",
    "                # state size. ``(nc) x 256 x 256``\n",
    "            )\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1689164151829,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "PyC-nI41XULT"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    def __init__(self, dims: list, embed_dim: int = 256, num_classes: int = 19):\n",
    "        super().__init__()\n",
    "        for i, dim in enumerate(dims):\n",
    "            self.add_module(f\"linear_c{i+1}\", MLP(dim, embed_dim))\n",
    "\n",
    "        self.linear_fuse = ConvModule(embed_dim*4, embed_dim)\n",
    "        self.linear_pred = nn.Conv2d(embed_dim, num_classes, 1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, features: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        B, _, H, W = features[0].shape\n",
    "        outs = [self.linear_c1(features[0]).permute(0, 2, 1).reshape(B, -1, *features[0].shape[-2:])]\n",
    "\n",
    "        for i, feature in enumerate(features[1:]):\n",
    "            cf = eval(f\"self.linear_c{i+2}\")(feature).permute(0, 2, 1).reshape(B, -1, *feature.shape[-2:])\n",
    "            outs.append(F.interpolate(cf, size=(H, W), mode='bilinear', align_corners=False))\n",
    "\n",
    "        seg = self.linear_fuse(torch.cat(outs[::-1], dim=1))\n",
    "        seg = self.linear_pred(self.dropout(seg))\n",
    "        return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1689164151831,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "MCJtu7Y_n-Cr"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, 1, -1)\n",
    "\n",
    "class FlattenFinal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1, 1, 1)\n",
    "\n",
    "class Conv2DModule(nn.Module):\n",
    "    def __init__(self, cin, cout, ks, s):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(cin, cout, ks, stride=s, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "class Conv1DModule(nn.Module):\n",
    "    def __init__(self, cin, cout, ks, s):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(cin, cout, ks, stride=s, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(cout)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "class downsample(nn.Module):\n",
    "    def __init__(self, C, H, W):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Sequential()\n",
    "        i = 0\n",
    "        alt = True\n",
    "        if H*W > 128:\n",
    "            while(C>4):\n",
    "\n",
    "                if H*W*C < 500:\n",
    "                    break\n",
    "\n",
    "                if H//2<3 or W//2<3:\n",
    "                    if alt:\n",
    "                        self.downsample.append(Flatten())\n",
    "                        C = C*H*W\n",
    "                        alt = False\n",
    "                    self.downsample.append(Conv1DModule(1, 1, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                elif H//2<9 or W//2<9:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H-2\n",
    "                    W = W-2\n",
    "                else:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 2))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H//2 -1\n",
    "                    W = W//2 -1\n",
    "\n",
    "        else:\n",
    "            while(C>16):\n",
    "\n",
    "                if H*W*C < 500:\n",
    "                    break\n",
    "\n",
    "                if H//2<3 or W//2<3:\n",
    "                    if alt:\n",
    "                        self.downsample.append(Flatten())\n",
    "                        C = C*H*W\n",
    "                        alt = False\n",
    "                    self.downsample.append(Conv1DModule(1, 1, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                elif H//2<9 or W//2<9:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H-2\n",
    "                    W = W-2\n",
    "                else:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 2))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H//2 -1\n",
    "                    W = W//2 -1\n",
    "        self.downsample.append(FlattenFinal())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.downsample(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1689164151833,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "ipHImK80n-Ct"
   },
   "outputs": [],
   "source": [
    "class SegFormerHeadGen(nn.Module):\n",
    "\n",
    "    def __init__(self, num_img):\n",
    "      self.num_img = num_img\n",
    "      super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, features: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        downsamplers = []\n",
    "        for x in features:\n",
    "            downsamplers.append(downsample(x.shape[1],x.shape[2],x.shape[3]).to(device))\n",
    "        \n",
    "        latents = torch.empty(features[0].shape[0],1,1,1).to(device)\n",
    "        \n",
    "        for i, feature in enumerate(features):            \n",
    "            latents = torch.cat( (latents, downsamplers[i](feature), latents), 1)\n",
    "    \n",
    "        image = Generator(latents.shape[1]).to(device)(latents)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1689164151835,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "cZsudwlzXULU"
   },
   "outputs": [],
   "source": [
    "from semseg.models.base import BaseModel\n",
    "from semseg.models.heads import SegFormerHead\n",
    "#from resDecode import ResDecode\n",
    "\n",
    "\n",
    "class SegFormerpp(BaseModel):\n",
    "    def __init__(self, backbone: str = 'MiT-B0', num_classes: int = 19, head: str = 'B0', num_img: int = -1) -> None:\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.head = head\n",
    "        self.decode = SegFormerHead(self.backbone.channels, 256 if 'B0' in backbone or 'B1' in backbone else 768, 3)\n",
    "        self.newDecode = SegFormerHeadGen(device)\n",
    "        self.newDecode.to(device)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.backbone(x)\n",
    "        #y = self.decode(y)\n",
    "        #y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        #embeds = []\n",
    "        y = self.newDecode(y)\n",
    "        '''\n",
    "        y = self.decode(y)\n",
    "        self.upsample = nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],self.num_classes//2,3,stride=2,output_padding=1, padding=1),\n",
    "        torch.nn.ConvTranspose2d(self.num_classes//2,self.num_classes,3,stride=2,output_padding=1, padding=1),\n",
    "                                )\n",
    "        y = self.upsample(y).to(device)\n",
    "        '''\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semseg.models.base import BaseModel\n",
    "from semseg.models.heads import SegFormerHead\n",
    "#from resDecode import ResDecode\n",
    "\n",
    "\n",
    "class SegNetpp(BaseModel):\n",
    "    def __init__(self, backbone: str = 'ResNet', num_classes: int = 19, head: str = 'B0', num_img: int = -1) -> None:\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.head = head\n",
    "        self.decode = SegFormerHead(self.backbone.channels, 256 if 'B0' in backbone or 'B1' in backbone else 768, 3)\n",
    "        self.newDecode = SegFormerHeadGen(num_img)\n",
    "        self.newDecode.to(device)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.backbone(x)\n",
    "        #y = self.decode(y)\n",
    "        #y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        #embeds = []\n",
    "        y = self.newDecode(y)\n",
    "        '''\n",
    "        y = self.decode(y)\n",
    "        self.upsample = nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],self.num_classes//2,3,stride=2,output_padding=1, padding=1),\n",
    "        torch.nn.ConvTranspose2d(self.num_classes//2,self.num_classes,3,stride=2,output_padding=1, padding=1),\n",
    "                                )\n",
    "        y = self.upsample(y).to(device)\n",
    "        '''\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1689164378457,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "bglipAaW_xaY",
    "outputId": "3e943d2f-8865-402a-df70-ec5bc156817f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download a pretrained model's weights from the result table.\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "from semseg.models import *\n",
    "#from segFormerpp import SegFormerpp\n",
    "\n",
    "model = eval('SegFormerpp')(\n",
    "    backbone='MiT-B2',\n",
    "    num_classes=5,\n",
    "    num_img = 0\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('checkpointcheckpoints/pretrained/segformer/segformer.b4.ade.pth'))\n",
    "except:\n",
    "    print(\"Download a pretrained model's weights from the result table.\")\n",
    "model.eval()\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28194,
     "status": "ok",
     "timestamp": 1689164179977,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "stv4_1XZXULW",
    "outputId": "ba951ed3-d81a-44e2-cae6-fecb989a9cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting parameters: \n",
      "             Dataset: ai4mars-dataset-merged-0.1 \n",
      "             Colab environment: False \n",
      "             Split percentages: [0.7, 0.2, 0.1] \n",
      "             Transformation: None \n",
      "             Svaving path: None \n",
      "             New image size: 128\n",
      "Extrapolation of random inices ...\n",
      "Splitting in progress ...\n",
      "Done \n",
      "\n",
      "Building Dataloaders\n",
      "Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "\n",
    "# Set this to True if you wnat to load directly the dataloader\n",
    "# this can be done only on colab and it is useful to avoid runtime crash\n",
    "LOAD = True\n",
    "\n",
    "if LOAD:\n",
    "\n",
    "    if COLAB:\n",
    "\n",
    "        if not(os.path.exists('/content/dataset/')):\n",
    "\n",
    "            import gdown\n",
    "\n",
    "            # get url of torch dataset (temporarerly my drive)\n",
    "            drive = 'https://drive.google.com/uc?id='\n",
    "            url = 'https://drive.google.com/drive/folders/104YvO3LcU76euuVe-_62eS_Rld-tOZeh?usp=drive_link'\n",
    "\n",
    "            !gdown --folder {url} -O /content/\n",
    "\n",
    "            load_data = '/content/dataset/dataset.pt'\n",
    "\n",
    "    elif LOCAL: \n",
    "        load_data = root_dir + '/datasetup/dataset/dataset1000.pt'\n",
    "\n",
    "    X, y = torch.load(load_data)\n",
    "\n",
    "    # Build dataset\n",
    "    splitter = Ai4MarsSplitter()\n",
    "    train_set, test_set, val_set = splitter(X, y, [0.7, 0.2, 0.1])\n",
    "\n",
    "    # Load dataset info\n",
    "    load_info = './.info.pt'\n",
    "    info = torch.load(load_info)\n",
    "\n",
    "    # Build Ai4MarsDataloader\n",
    "    loader = Ai4MarsDataLoader()\n",
    "    train_loader, test_loader, val_loader = loader(\n",
    "        [train_set, test_set, val_set], [64, 32, 32])\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    # Insert here your local path to the dataset (temporary)\n",
    "    data_path = \"/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/\" #input(\"Path to Dataset: \")\n",
    "\n",
    "    # Insert here the number of images you want to download\n",
    "    num_images = 100 #int(input(\"Number of images (max 1000): \"))\n",
    "\n",
    "    save_path = None\n",
    "    # Uncomment the following line to the dataset on a local path\n",
    "    #save_path = root_dir + '/datasetup/dataset/'\n",
    "\n",
    "    if num_images > 1000 : raise Exception(\"Trying to import too many images\")\n",
    "\n",
    "    # Import data as Ai4MarsDataset\n",
    "    Ai4MarsDownload()(PATH=data_path)\n",
    "    importer = Ai4MarsImporter()\n",
    "    X, y, _ = importer(PATH=data_path, NUM_IMAGES=num_images, SAVE_PATH=save_path, SIZE=128)\n",
    "\n",
    "    transform = None\n",
    "    # Uncomment the following lines to apply transformations to the dataset\n",
    "    '''\n",
    "    transform = transforms.RandomChoice([\n",
    "     transforms.RandomRotation(90)])\n",
    "    '''\n",
    "\n",
    "    # Load info\n",
    "    load_info = './.info.pt'\n",
    "    info = torch.load(load_info)\n",
    "    \n",
    "    # Split the dataset\n",
    "    splitter = Ai4MarsSplitter()\n",
    "    train_set, test_set, val_set = splitter(X, y, [0.7, 0.2, 0.1], transform=transform,\n",
    "                                            SAVE_PATH=save_path)\n",
    "\n",
    "    # Build Ai4MarsDataloader\n",
    "    loader = Ai4MarsDataLoader()\n",
    "    train_loader, test_loader, val_loader = loader([train_set, test_set, val_set], [32, 16, 16],\n",
    "                                                   SAVE_PATH=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1689164179981,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "O3ytN-ugjHcW",
    "outputId": "29fe6dcc-1983-4c1b-a1e5-eabff10ea398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n  print(img.device)\\n  print(img.shape)\\n  if (img.device != 'cpu'):\\n    img.to('cpu')\\n    print('test')\\n  print(img.device)\\n  plt.imshow(img)\\n  plt.show()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show_image(imgs):\n",
    "    if len(imgs.size()) == 4:\n",
    "        for img in imgs:\n",
    "            imgs = imgs.permute(2,0,1)\n",
    "    else:\n",
    "        imgs = imgs.permute(2,0,1)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "'''\n",
    "  print(img.device)\n",
    "  print(img.shape)\n",
    "  if (img.device != 'cpu'):\n",
    "    img.to('cpu')\n",
    "    print('test')\n",
    "  print(img.device)\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1689164179986,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "3lU21D6nXULX"
   },
   "outputs": [],
   "source": [
    "\"\"\" Trainer module for MER-Segmentation 2.0 \"\"\"\n",
    "test2 = False\n",
    "if test2:\n",
    "\n",
    "    import time\n",
    "    from datetime import datetime \n",
    "\n",
    "    # This class collects all the training functionalities to train different models\n",
    "    class Ai4MarsTrainer():\n",
    "\n",
    "        # Initialization of training parameters in the class constructor\n",
    "        def __init__(self, loss_fn, optimizer, train_loader, val_loader,\n",
    "                     transform=None, device='cpu', save_state=None):\n",
    "            self.loss_fn = loss_fn\n",
    "            self.optimizer = optimizer\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.device = device\n",
    "            self.save_state = save_state\n",
    "            self.transform = transform\n",
    "            self.loss_list = []\n",
    "            self.tloss_list = []\n",
    "\n",
    "        # This function implements training for just one epoch\n",
    "        def train_one_epoch(self, model, epoch_index=0):\n",
    "            running_loss = 0.\n",
    "            last_loss = 0.\n",
    "\n",
    "            # parameters for online data augmentation (batch transformation)\n",
    "            running_tloss = 0.\n",
    "            last_tloss = 0.\n",
    "            t_index = 0 \n",
    "\n",
    "            for batch_index, batch in enumerate(self.train_loader):\n",
    "                # Every data instance is an (input, label) pair\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Zero your gradients for every batch!\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Send inputs and labels to GPU (or whatever device is)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Adjust label to be 2D tensors of batch size\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels.long()\n",
    "\n",
    "                # Compute the loss and its gradients\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                # Adjust learning weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # if transformation exists apply them to the batch\n",
    "                if self.transform:\n",
    "                    tinputs = self.transform(inputs)\n",
    "                    tinputs = tinputs.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    toutputs = model(tinputs)\n",
    "                    tloss = self.loss_fn(toutputs, labels)\n",
    "                    tloss.backward()\n",
    "                    running_tloss = tloss.item()\n",
    "                    t_index += 1\n",
    "                    tinputs.detach()\n",
    "                    del tinputs\n",
    "\n",
    "                # Free up RAM/VRAM\n",
    "                inputs.detach()\n",
    "                labels.detach()\n",
    "                del inputs\n",
    "                del labels\n",
    "\n",
    "            # Compute the average loss over all batches\n",
    "            last_loss =  running_loss / (batch_index + 1)\n",
    "\n",
    "            # Print report at the end of the last batch\n",
    "            print(f'Epoch {epoch_index+1}')\n",
    "            print(f'LOSS ON TRAIN: {last_loss}')\n",
    "\n",
    "            if self.transform:\n",
    "                last_tloss = running_tloss / (t_index + 1)\n",
    "                print(f'LOSS ON TRANSFORMED-TRAIN: {last_tloss}')\n",
    "\n",
    "            else:\n",
    "                last_tloss = None\n",
    "\n",
    "            return last_loss, last_tloss\n",
    "\n",
    "        # This function implements training for multiple epochs\n",
    "        def train_multiple_epoch(self, model, EPOCHS=100):\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            epoch_number = 0 # just a counter\n",
    "            best_vloss = 1_000_000.\n",
    "            self.loss_list = []\n",
    "            self.tloss_list = []\n",
    "\n",
    "            for epoch in range(EPOCHS):\n",
    "                # Make sure gradient tracking is on, and do a pass over the data\n",
    "                model.train(True)\n",
    "\n",
    "                # Start monitoring training time\n",
    "                start = time.time()\n",
    "\n",
    "                avg_loss = self.train_one_epoch(model, epoch)\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                # We don't need gradients on to do reporting\n",
    "                model.train(False)\n",
    "\n",
    "                # Test loss\n",
    "                running_vloss = 0.0\n",
    "                for vbatch_index, vbatch in enumerate(self.val_loader):\n",
    "\n",
    "                    # Every data instance is a (input, label) pair\n",
    "                    vinputs, vlabels = vbatch\n",
    "\n",
    "                    # Send inputs and labels to GPU\n",
    "                    vinputs = vinputs.to(self.device)\n",
    "                    vlabels = vlabels.to(self.device)\n",
    "\n",
    "                    # Model prediction\n",
    "                    voutputs = model(vinputs)\n",
    "\n",
    "                    # Send inputs and labels to GPU (or whatever device is)\n",
    "                    vlabels = vlabels.squeeze()\n",
    "                    vlabels = vlabels.long()\n",
    "\n",
    "                    # Run validation loss\n",
    "                    val_loss = self.loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += val_loss.item()\n",
    "\n",
    "                    # Free up RAM/VRAM\n",
    "                    vinputs.detach()\n",
    "                    vlabels.detach()\n",
    "                    del vinputs\n",
    "                    del vlabels\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Compute the average loss over all batches\n",
    "                avg_vloss = running_vloss / (vbatch_index + 1)\n",
    "\n",
    "                print(\"Time needed for training: \" + str(end-start)+ \" seconds\")\n",
    "\n",
    "                # Print report at the end of the epoch\n",
    "                print(f'LOSS ON VALIDATION: {avg_vloss}')\n",
    "\n",
    "                # Save loss in a list to then perform metrics evaluation\n",
    "                self.loss_list.append((avg_loss[0], end-start))\n",
    "\n",
    "                # If online data augmentation has been performed:\n",
    "                if avg_loss[1]:\n",
    "                    self.tloss_list.append((avg_loss[1], end-start))\n",
    "\n",
    "                # Track best performance, and save the model's state\n",
    "                if avg_vloss < best_vloss:\n",
    "                    best_vloss = avg_vloss\n",
    "                    model_path = self.save_state + 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test2:\n",
    "    from typing import Optional\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    # based on:\n",
    "    # https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n",
    "\n",
    "    class DiceLoss(nn.Module):\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            super(DiceLoss, self).__init__()\n",
    "            self.eps: float = 1e-6\n",
    "\n",
    "        def forward(\n",
    "                self,\n",
    "                input: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\n",
    "            #errors:\n",
    "            if not torch.is_tensor(input):\n",
    "                raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                                .format(type(input)))\n",
    "            if not len(input.shape) == 4:\n",
    "                raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                                 .format(input.shape))\n",
    "            if not input.shape[-2:] == target.shape[-2:]:\n",
    "                raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                                 .format(input.shape, input.shape))\n",
    "            if not input.device == target.device:\n",
    "                raise ValueError(\n",
    "                    \"input and target must be in the same device. Got: {}\" .format(\n",
    "                        input.device, target.device))\n",
    "\n",
    "\n",
    "            num_classes = input.shape[1]\n",
    "\n",
    "            # compute softmax over the classes axis\n",
    "            input_soft = F.softmax(input, dim=1)\n",
    "\n",
    "            # print(\"shape\", input_soft.shape)\n",
    "\n",
    "            input_soft = input_soft.permute(0,2,1,3)\n",
    "            input_soft = input_soft.permute(0,1,3,2)\n",
    "\n",
    "            # create the labels one hot tensor\n",
    "            target_one_hot = F.one_hot(target, num_classes=input.shape[1])\n",
    "\n",
    "            # target_one_hot = target_one_hot.permute(0,1,3,2)\n",
    "            # target_one_hot = target_one_hot.permute(0,2,1,3)\n",
    "\n",
    "            # print(\"target_one_hot\",target_one_hot.shape)\n",
    "            # print(\"input_soft\",input_soft.shape)\n",
    "\n",
    "\n",
    "            # compute the actual dice score\n",
    "            dims = (1, 2, 3)\n",
    "            intersection = torch.sum(input_soft.reshape(-1) * target_one_hot.reshape(-1), -1)\n",
    "            cardinality = torch.sum(input_soft.reshape(-1) + target_one_hot.reshape(-1), -1)\n",
    "\n",
    "            dice_score = 2. * intersection / (cardinality + self.eps)\n",
    "\n",
    "            # if (self.stampa):\n",
    "            #   print(\"outputs\",input_soft)\n",
    "            #   print(\"labels\",target_one_hot)\n",
    "            #   print((1. - dice_score).item())\n",
    "\n",
    "\n",
    "\n",
    "            # print((1. - dice_score).shape)\n",
    "\n",
    "\n",
    "\n",
    "            return 1. - dice_score\n",
    "\n",
    "            #Alternative Universe\n",
    "\n",
    "            # labels = target_one_hot\n",
    "            # preds = input_soft\n",
    "\n",
    "            # tp = torch.sum(labels*preds, dim=(2, 3))\n",
    "            # fn = torch.sum(labels*(1-preds), dim=(2, 3))\n",
    "            # fp = torch.sum((1-labels)*preds, dim=(2, 3))\n",
    "\n",
    "\n",
    "            # delta = 0.5 # va messo come hyperparameter\n",
    "\n",
    "            # dice_score = (tp + 1e-6) / (tp + delta * fn + (1 - delta) * fp + 1e-6)\n",
    "            # dice_score = torch.sum(1 - dice_score, dim=-1)\n",
    "\n",
    "            # dice_score = dice_score / num_classes\n",
    "            # return dice_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVll-qRSXULY",
    "outputId": "e125181d-a822-4c69-f272-ea55ec43af18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 13894, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Ai4MarsTrainer(loss_fn_dice, optimizer, train_loader, val_loader, lr_scheduler, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multiple_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Plot the histogram\u001b[39;00m\n\u001b[1;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mparam_hist(model)\n",
      "File \u001b[0;32m~/Downloads/magistrale/1 anno/visiope/visiope/tools/trainer/trainer.py:172\u001b[0m, in \u001b[0;36mAi4MarsTrainer.train_multiple_epoch\u001b[0;34m(self, model, EPOCHS, SAVE_PATH)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Start monitoring training time\u001b[39;00m\n\u001b[1;32m    170\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 172\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# End of monitoring time\u001b[39;00m\n\u001b[1;32m    175\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Downloads/magistrale/1 anno/visiope/visiope/tools/trainer/trainer.py:59\u001b[0m, in \u001b[0;36mAi4MarsTrainer.train_one_epoch\u001b[0;34m(self, model, epoch_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# New shape: B x W x H x C\u001b[39;00m\n\u001b[1;32m     62\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m, in \u001b[0;36mSegFormerpp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#y = self.decode(y)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#embeds = []\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewDecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03my = self.decode(y)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mself.upsample = nn.Sequential(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03my = self.upsample(y).to(device)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mSegFormerHeadGen.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     16\u001b[0m     latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat( (latents, downsamplers[i](feature), latents), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(latents\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 20\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)(latents)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self, nz)\u001b[0m\n\u001b[1;32m      7\u001b[0m nc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# input is Z, going into a convolution\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#in_channels, out_channels, kernel_size, stride=1, padding=0,\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvTranspose2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m     13\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     14\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf*8) x 4 x 4``\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConvTranspose2d(ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m, ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     18\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf*4) x 8 x 8``\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConvTranspose2d( ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     21\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf*2) x 16 x 16``\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConvTranspose2d( ngf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, ngf, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     25\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(ngf),\n\u001b[1;32m     26\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf) x 32 x 32``\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConvTranspose2d( ngf, ngf\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     29\u001b[0m     nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(ngf\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     30\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf/2) x 64 x 64``\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConvTranspose2d( ngf\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, nc, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     33\u001b[0m     nn\u001b[38;5;241m.\u001b[39mTanh()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngc) x 128 x 128``\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# state size. ``(ngf/4) x 128 x 128``\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#nn.BatchNorm2d(ngf),\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#nn.ReLU(True),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#nn.ConvTranspose2d( ngf/4, nc, 4, 2, 1, bias=False),\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#nn.Tanh()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# state size. ``(nc) x 256 x 256``\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:940\u001b[0m, in \u001b[0;36mConvTranspose2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias, dilation, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m dilation \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m    939\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m _pair(output_padding)\n\u001b[0;32m--> 940\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:625\u001b[0m, in \u001b[0;36m_ConvTransposeNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m padding mode is supported for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m    624\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:89\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups must be a positive integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_channels \u001b[38;5;241m%\u001b[39m groups \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_channels must be divisible by groups\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_channels \u001b[38;5;241m%\u001b[39m groups \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "                {'params': model.backbone.parameters(), 'lr': 1e1},\n",
    "                {'params': model.decode.parameters(), 'lr': 1e-3}\n",
    "                ]\n",
    "                            ,amsgrad = True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 500)\n",
    "loss_fn_cross = Ai4MarsCrossEntropy().to(device)\n",
    "loss_fn_dice = Ai4MarsDiceLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = Ai4MarsTrainer(loss_fn_dice, optimizer, train_loader, val_loader, lr_scheduler, device = device)\n",
    "model.to(device)\n",
    "trainer.train_multiple_epoch(model, EPOCHS = 1400)\n",
    "# Plot the histogram\n",
    "trainer.param_hist(model)\n",
    "trainer.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "error",
     "timestamp": 1689164347160,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "FZzOcvNXLK19",
    "outputId": "6d586d17-2309-43bf-e0a8-02c5aa9036d4"
   },
   "outputs": [],
   "source": [
    "from semseg.datasets import *\n",
    "\n",
    "#model = model.to(device)\n",
    "predictions = []\n",
    "start = time.time()\n",
    "#print('test')\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move the data to the desired device\n",
    "        #print(images.shape)\n",
    "        #images = images.permute(0,3,1,2).to(device)\n",
    "        #images = images.permute(2,0,1).to(device)\n",
    "        #images = images [None, :, :, :]\n",
    "        #print(images.shape)\n",
    "\n",
    "        #print(images.shape)\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "        # Forward pass to get the predictions\n",
    "        with torch.inference_mode():\n",
    "          prediction = model(images)\n",
    "        #print(prediction)\n",
    "        prediction = prediction.softmax(1).argmax(1).to(int)\n",
    "        \n",
    "        #prediction = prediction.round().to(int)\n",
    "        #print(prediction.shape)\n",
    "        un = prediction.unique()\n",
    "        #print(un)\n",
    "        palette = eval('ADE20K').PALETTE.to(device)\n",
    "        prediction_map = palette[prediction].squeeze().to(torch.uint8)\n",
    "        #print(type(prediction_map))\n",
    "        #show_image(prediction_map)\n",
    "        predictions.append(prediction_map)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1689164347161,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "UKfyLsOXXBjx"
   },
   "outputs": [],
   "source": [
    "image, label = test_loader.dataset.__getitem__(10)\n",
    "\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "output = model(image)\n",
    "\n",
    "output = output.permute(0,2,1,3).permute(0,1,3,2)\n",
    "\n",
    "label = label.squeeze()\n",
    "label = label.long()\n",
    "\n",
    "print(loss_fn_dice(output, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(64, 324, 1, 1)\n",
    "x2 = torch.randn(64, 144, 1, 1)\n",
    "x3 = torch.randn(64, 1268, 1, 1)\n",
    "x4 = torch.randn(64, 8174, 1, 1)\n",
    "x0 = torch.empty(64,1,1,1)\n",
    "\n",
    "\n",
    "x0 = torch.cat((x0, x1 ,x0),1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

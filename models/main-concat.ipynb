{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3551,
     "status": "ok",
     "timestamp": 1689164151821,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "kytW-DGR_xaS",
    "outputId": "e2d0b3eb-5cdd-45d6-af7e-00fb52eb82f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIN_COLAB = \\'google.colab\\' in sys.modules\\n\\nif not IN_COLAB:\\n    from git import Repo\\n\\n    # Initialize the Git repository object\\n    repo = Repo(\".\", search_parent_directories=True)\\n\\n    # Get the root directory of the Git project\\n    root_dir = repo.git.rev_parse(\"--show-toplevel\")\\n\\n    from pathlib import Path\\n\\n    # Set up path for custom importer modules\\n    # Data Loader\\n    importer_module = root_dir + \\'/dataloader/\\'\\n    print(importer_module)\\n    sys.path.insert(0, importer_module)\\n    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\\n\\n    # Loss\\n\\n    loss_module = root_dir + \\'/trainer/loss/\\'\\n    sys.path.insert(0, loss_module)\\n    import loss\\n\\n    # Trainer\\n    trainer_module = root_dir + \\'/trainer/\\'\\n    sys.path.insert(0, trainer_module)\\n    from trainer import Ai4MarsTrainer\\n\\n    # Insert here your local path to the dataset (temporary)\\n    data_path = input(\"Path to Dataset: \") #\\'/home/leeoos/Desktop/\\'\\n\\nelse: # IN_COLAB\\n\\n    from google.colab import drive\\n    drive.mount(\\'/content/drive\\')\\n\\n    # On Colab the path to the module ti fixed once you have\\n    # corretly set up the project with gitsetup.ipynb\\n\\n    # Import Loader\\n    fixed_path_loader = \\'/content/drive/MyDrive/Github/visiope/dataloader/\\'\\n    sys.path.insert(0, fixed_path_loader)\\n    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\\n\\n    # Import Trainer\\n    fixed_path_trainer = \\'/content/drive/MyDrive/Github/visiope/trainer/\\'\\n    sys.path.insert(0,fixed_path_trainer )\\n    from trainer import Ai4MarsTrainer\\n\\n    # Import Loss\\n    fixed_path_loss = \\'/content/drive/MyDrive/Github/visiope/trainer/loss/\\'\\n    sys.path.insert(0, fixed_path_loss)\\n    import loss\\n    \\n    !git clone https://github.com/sithu31296/semantic-segmentation\\n    %pip install -U gdown\\n    %pip install -e .\\n    %pip install einops\\n    import gdown\\n    from pathlib import Path\\n\\n    ckpt = Path(\\'./checkpoints/pretrained/segformer\\')\\n    ckpt.mkdir(exist_ok=True, parents=True)\\n\\n    url = \\'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT\\'\\n    output = \\'./checkpoints/pretrained/segformer/segformer.b3.ade.pth\\'\\n\\n    gdown.download(url, output, quiet=False)\\n\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n%cd semantic-segmentation\\n#/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/dataloader/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import io\n",
    "from torchvision import transforms \n",
    "from PIL import Image\n",
    "import pickle\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import gc\n",
    "from typing import Tuple\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "'''\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if not IN_COLAB:\n",
    "    from git import Repo\n",
    "\n",
    "    # Initialize the Git repository object\n",
    "    repo = Repo(\".\", search_parent_directories=True)\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Set up path for custom importer modules\n",
    "    # Data Loader\n",
    "    importer_module = root_dir + '/dataloader/'\n",
    "    print(importer_module)\n",
    "    sys.path.insert(0, importer_module)\n",
    "    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n",
    "\n",
    "    # Loss\n",
    "\n",
    "    loss_module = root_dir + '/trainer/loss/'\n",
    "    sys.path.insert(0, loss_module)\n",
    "    import loss\n",
    "\n",
    "    # Trainer\n",
    "    trainer_module = root_dir + '/trainer/'\n",
    "    sys.path.insert(0, trainer_module)\n",
    "    from trainer import Ai4MarsTrainer\n",
    "\n",
    "    # Insert here your local path to the dataset (temporary)\n",
    "    data_path = input(\"Path to Dataset: \") #'/home/leeoos/Desktop/'\n",
    "\n",
    "else: # IN_COLAB\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # On Colab the path to the module ti fixed once you have\n",
    "    # corretly set up the project with gitsetup.ipynb\n",
    "\n",
    "    # Import Loader\n",
    "    fixed_path_loader = '/content/drive/MyDrive/Github/visiope/dataloader/'\n",
    "    sys.path.insert(0, fixed_path_loader)\n",
    "    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n",
    "\n",
    "    # Import Trainer\n",
    "    fixed_path_trainer = '/content/drive/MyDrive/Github/visiope/trainer/'\n",
    "    sys.path.insert(0,fixed_path_trainer )\n",
    "    from trainer import Ai4MarsTrainer\n",
    "\n",
    "    # Import Loss\n",
    "    fixed_path_loss = '/content/drive/MyDrive/Github/visiope/trainer/loss/'\n",
    "    sys.path.insert(0, fixed_path_loss)\n",
    "    import loss\n",
    "    \n",
    "    !git clone https://github.com/sithu31296/semantic-segmentation\n",
    "    %pip install -U gdown\n",
    "    %pip install -e .\n",
    "    %pip install einops\n",
    "    import gdown\n",
    "    from pathlib import Path\n",
    "\n",
    "    ckpt = Path('./checkpoints/pretrained/segformer')\n",
    "    ckpt.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n",
    "    output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n",
    "\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%cd semantic-segmentation\n",
    "#/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/dataloader/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/models/semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "# Custom Imports\n",
    "\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "LOCAL = not COLAB\n",
    "\n",
    "if COLAB:\n",
    "\n",
    "    # Clone visiope repo on runtime env\n",
    "    !git clone https://github.com/airoprojects/visiope.git /\n",
    "\n",
    "    # Install pytorchmetrics\n",
    "    !pip install torchmetrics\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = '/content/visiope'\n",
    "\n",
    "    # Add custom modules to path\n",
    "    custom_modules_path = root_dir + '/tools/'\n",
    "    sys.path.insert(0, custom_modules_path)\n",
    "\n",
    "elif LOCAL:\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "    from git import Repo\n",
    "\n",
    "    # Initialize the Git repository object\n",
    "    repo = Repo(\".\", search_parent_directories=True)\n",
    "\n",
    "    # Get the root directory of the Git project\n",
    "    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "    # Add custom modules to path\n",
    "    custom_modules_path = root_dir  + '/tools/'\n",
    "    sys.path.insert(0, custom_modules_path)\n",
    "\n",
    "\n",
    "# Import Loader\n",
    "from data.utils import Ai4MarsDownload, Ai4MarsSplitter, Ai4MarsDataLoader\n",
    "\n",
    "# Import Loss\n",
    "from loss.loss import Ai4MarsCrossEntropy, Ai4MarsDiceLoss\n",
    "\n",
    "# Import Trainer\n",
    "from trainer.trainer import Ai4MarsTrainer\n",
    "\n",
    "# Import Tester\n",
    "from tester.tester import Ai4MarsTester\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%cd semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1689164151824,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "kwVFZJKgXULT"
   },
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, nc):\n",
    "            self.nc = nc\n",
    "            super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "        def forward(self, embeds):\n",
    "            ngf = 64\n",
    "            nz = [[None] *6]*4\n",
    "\n",
    "            j = 0\n",
    "            for embed in embeds:\n",
    "                nz[j][0] = embed.size()[1]\n",
    "                for i in range(1,6):\n",
    "                    if i < 4:\n",
    "                        nz[j][i] = nz[i-1]//2\n",
    "                    else:\n",
    "                        nz[j][i] = nz[i-1]//4\n",
    "                j = j+1\n",
    "                print(nz)\n",
    "            self.main = nn.Sequential(\n",
    "                #reduction of dimensionality To Be Changed in conv... maybe\n",
    "                # input is Z, going into a convolution\n",
    "                nn.ConvTranspose2d( nz[6], ngf * 8, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 8),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*8) x 4 x 4``\n",
    "                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*4) x 8 x 8``\n",
    "                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*2) x 16 x 16``\n",
    "                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf) x 32 x 32``\n",
    "                nn.ConvTranspose2d( ngf, self.nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh()\n",
    "                # state size. ``(nc) x 64 x 64``\n",
    "            )\n",
    "\n",
    "            return self.main(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1689164151825,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "1n9BBXOOn-Cl"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super(Generator, self).__init__()\n",
    "        ngf = 64\n",
    "        nc = 1\n",
    "        print(nz)\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1689164151827,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "ZD2IYalLn-Co"
   },
   "outputs": [],
   "source": [
    "test1 = True\n",
    "if test1:\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, nz):\n",
    "            super(Generator, self).__init__()\n",
    "            ngf = 64\n",
    "            nc = 5\n",
    "            self.main = nn.Sequential()\n",
    "            self.main = nn.Sequential(\n",
    "                # input is Z, going into a convolution\n",
    "                #in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 8),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*8) x 4 x 4``\n",
    "                nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*4) x 8 x 8``\n",
    "                nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf * 2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf*2) x 16 x 16``\n",
    "                nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf) x 32 x 32``\n",
    "                nn.ConvTranspose2d( ngf, ngf//2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ngf//2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. ``(ngf/2) x 64 x 64``\n",
    "                nn.ConvTranspose2d( ngf//2, nc, 4, 2, 1, bias=False),\n",
    "                nn.Tanh()\n",
    "                # state size. ``(ngc) x 128 x 128``\n",
    "                # state size. ``(ngf/4) x 128 x 128``\n",
    "                #nn.BatchNorm2d(ngf),\n",
    "                #nn.ReLU(True),\n",
    "                #nn.ConvTranspose2d( ngf/4, nc, 4, 2, 1, bias=False),\n",
    "                #nn.Tanh()\n",
    "                # state size. ``(nc) x 256 x 256``\n",
    "            )\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1689164151829,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "PyC-nI41XULT"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    def __init__(self, dims: list, embed_dim: int = 256, num_classes: int = 19):\n",
    "        super().__init__()\n",
    "        for i, dim in enumerate(dims):\n",
    "            self.add_module(f\"linear_c{i+1}\", MLP(dim, embed_dim))\n",
    "\n",
    "        self.linear_fuse = ConvModule(embed_dim*4, embed_dim)\n",
    "        self.linear_pred = nn.Conv2d(embed_dim, num_classes, 1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, features: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        B, _, H, W = features[0].shape\n",
    "        outs = [self.linear_c1(features[0]).permute(0, 2, 1).reshape(B, -1, *features[0].shape[-2:])]\n",
    "\n",
    "        for i, feature in enumerate(features[1:]):\n",
    "            cf = eval(f\"self.linear_c{i+2}\")(feature).permute(0, 2, 1).reshape(B, -1, *feature.shape[-2:])\n",
    "            outs.append(F.interpolate(cf, size=(H, W), mode='bilinear', align_corners=False))\n",
    "\n",
    "        seg = self.linear_fuse(torch.cat(outs[::-1], dim=1))\n",
    "        seg = self.linear_pred(self.dropout(seg))\n",
    "        return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1689164151831,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "MCJtu7Y_n-Cr"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, 1, -1)\n",
    "\n",
    "class FlattenFinal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1, 1, 1)\n",
    "\n",
    "class Conv2DModule(nn.Module):\n",
    "    def __init__(self, cin, cout, ks, s):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(cin, cout, ks, stride=s, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "class Conv1DModule(nn.Module):\n",
    "    def __init__(self, cin, cout, ks, s):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(cin, cout, ks, stride=s, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(cout)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "class downsample(nn.Module):\n",
    "    def __init__(self, C, H, W):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Sequential()\n",
    "        i = 0\n",
    "        alt = True\n",
    "        if H*W > 128:\n",
    "            while(C>4):\n",
    "\n",
    "                if H*W*C < 500:\n",
    "                    break\n",
    "\n",
    "                if H//2<3 or W//2<3:\n",
    "                    if alt:\n",
    "                        self.downsample.append(Flatten())\n",
    "                        C = C*H*W\n",
    "                        alt = False\n",
    "                    self.downsample.append(Conv1DModule(1, 1, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                elif H//2<9 or W//2<9:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H-2\n",
    "                    W = W-2\n",
    "                else:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 2))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H//2 -1\n",
    "                    W = W//2 -1\n",
    "\n",
    "        else:\n",
    "            while(C>16):\n",
    "\n",
    "                if H*W*C < 500:\n",
    "                    break\n",
    "\n",
    "                if H//2<3 or W//2<3:\n",
    "                    if alt:\n",
    "                        self.downsample.append(Flatten())\n",
    "                        C = C*H*W\n",
    "                        alt = False\n",
    "                    self.downsample.append(Conv1DModule(1, 1, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                elif H//2<9 or W//2<9:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 1))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H-2\n",
    "                    W = W-2\n",
    "                else:\n",
    "                    self.downsample.append(Conv2DModule(C, C//2, 3, 2))\n",
    "                    C = C//2\n",
    "                    i = i+1\n",
    "                    H = H//2 -1\n",
    "                    W = W//2 -1\n",
    "        self.downsample.append(FlattenFinal())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.downsample(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1689164151833,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "ipHImK80n-Ct"
   },
   "outputs": [],
   "source": [
    "class SegFormerHeadGen(nn.Module):\n",
    "\n",
    "    def __init__(self, num_img):\n",
    "      self.num_img = num_img\n",
    "      super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, features: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "      x = features[self.num_img]\n",
    "      downsampler = downsample(x.shape[1],x.shape[2],x.shape[3]).to(device)\n",
    "\n",
    "\n",
    "      downsampler(x).size()\n",
    "      self.G = Generator(downsampler(x).size()[1]).to(device)\n",
    "      image = self.G(downsampler(x))\n",
    "\n",
    "\n",
    "      return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1689164151835,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "cZsudwlzXULU"
   },
   "outputs": [],
   "source": [
    "from semseg.models.base import BaseModel\n",
    "from semseg.models.heads import SegFormerHead\n",
    "#from resDecode import ResDecode\n",
    "\n",
    "\n",
    "class SegFormerpp(BaseModel):\n",
    "    def __init__(self, backbone: str = 'MiT-B0', num_classes: int = 19, head: str = 'B0', num_img: int = -1) -> None:\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.head = head\n",
    "        self.decode = SegFormerHead(self.backbone.channels, 256 if 'B0' in backbone or 'B1' in backbone else 768, 3)\n",
    "        self.newDecode = SegFormerHeadGen(num_img)\n",
    "        self.newDecode.to(device)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.backbone(x)\n",
    "        #y = self.decode(y)\n",
    "        #y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        #embeds = []\n",
    "        y = self.newDecode(y)\n",
    "        '''\n",
    "        y = self.decode(y)\n",
    "        self.upsample = nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],self.num_classes//2,3,stride=2,output_padding=1, padding=1),\n",
    "        torch.nn.ConvTranspose2d(self.num_classes//2,self.num_classes,3,stride=2,output_padding=1, padding=1),\n",
    "                                )\n",
    "        y = self.upsample(y).to(device)\n",
    "        '''\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semseg.models.base import BaseModel\n",
    "from semseg.models.heads import SegFormerHead\n",
    "#from resDecode import ResDecode\n",
    "\n",
    "\n",
    "class SegNetpp(BaseModel):\n",
    "    def __init__(self, backbone: str = 'MiT-B0', num_classes: int = 19, head: str = 'B0', num_img: int = -1) -> None:\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.head = head\n",
    "        self.decode = SegFormerHead(self.backbone.channels, 256 if 'B0' in backbone or 'B1' in backbone else 768, 3)\n",
    "        self.newDecode = SegFormerHeadGen(num_img)\n",
    "        self.newDecode.to(device)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.backbone(x)\n",
    "        #y = self.decode(y)\n",
    "        #y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        #embeds = []\n",
    "        y = self.newDecode(y)\n",
    "        '''\n",
    "        y = self.decode(y)\n",
    "        self.upsample = nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],self.num_classes//2,3,stride=2,output_padding=1, padding=1),\n",
    "        torch.nn.ConvTranspose2d(self.num_classes//2,self.num_classes,3,stride=2,output_padding=1, padding=1),\n",
    "                                )\n",
    "        y = self.upsample(y).to(device)\n",
    "        '''\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1689164378457,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "bglipAaW_xaY",
    "outputId": "3e943d2f-8865-402a-df70-ec5bc156817f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download a pretrained model's weights from the result table.\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "from semseg.models import *\n",
    "#from segFormerpp import SegFormerpp\n",
    "\n",
    "model = eval('SegFormerpp')(\n",
    "    backbone='MiT-B2',\n",
    "    num_classes=5,\n",
    "    num_img = 0\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('checkpointcheckpoints/pretrained/segformer/segformer.b4.ade.pth'))\n",
    "except:\n",
    "    print(\"Download a pretrained model's weights from the result table.\")\n",
    "model.eval()\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28194,
     "status": "ok",
     "timestamp": 1689164179977,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "stv4_1XZXULW",
    "outputId": "ba951ed3-d81a-44e2-cae6-fecb989a9cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting parameters: \n",
      "             Dataset: ai4mars-dataset-merged-0.1 \n",
      "             Colab environment: False \n",
      "             Split percentages: [0.7, 0.2, 0.1] \n",
      "             Transformation: None \n",
      "             Svaving path: None \n",
      "             New image size: 128\n",
      "Extrapolation of random inices ...\n",
      "Splitting in progress ...\n",
      "Done \n",
      "\n",
      "Building Dataloaders\n",
      "Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "\n",
    "# Set this to True if you wnat to load directly the dataloader\n",
    "# this can be done only on colab and it is useful to avoid runtime crash\n",
    "LOAD = True\n",
    "\n",
    "if LOAD:\n",
    "\n",
    "    if COLAB:\n",
    "\n",
    "        if not(os.path.exists('/content/dataset/')):\n",
    "\n",
    "            import gdown\n",
    "\n",
    "            # get url of torch dataset (temporarerly my drive)\n",
    "            drive = 'https://drive.google.com/uc?id='\n",
    "            url = 'https://drive.google.com/drive/folders/104YvO3LcU76euuVe-_62eS_Rld-tOZeh?usp=drive_link'\n",
    "\n",
    "            !gdown --folder {url} -O /content/\n",
    "\n",
    "            load_data = '/content/dataset/dataset.pt'\n",
    "\n",
    "    elif LOCAL: \n",
    "        load_data = root_dir + '/datasetup/dataset/dataset1000.pt'\n",
    "\n",
    "    X, y = torch.load(load_data)\n",
    "\n",
    "    # Build dataset\n",
    "    splitter = Ai4MarsSplitter()\n",
    "    train_set, test_set, val_set = splitter(X, y, [0.7, 0.2, 0.1])\n",
    "\n",
    "    # Load dataset info\n",
    "    load_info = './.info.pt'\n",
    "    info = torch.load(load_info)\n",
    "\n",
    "    # Build Ai4MarsDataloader\n",
    "    loader = Ai4MarsDataLoader()\n",
    "    train_loader, test_loader, val_loader = loader(\n",
    "        [train_set, test_set, val_set], [64, 32, 32])\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    # Insert here your local path to the dataset (temporary)\n",
    "    data_path = \"/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/\" #input(\"Path to Dataset: \")\n",
    "\n",
    "    # Insert here the number of images you want to download\n",
    "    num_images = 100 #int(input(\"Number of images (max 1000): \"))\n",
    "\n",
    "    save_path = None\n",
    "    # Uncomment the following line to the dataset on a local path\n",
    "    #save_path = root_dir + '/datasetup/dataset/'\n",
    "\n",
    "    if num_images > 1000 : raise Exception(\"Trying to import too many images\")\n",
    "\n",
    "    # Import data as Ai4MarsDataset\n",
    "    Ai4MarsDownload()(PATH=data_path)\n",
    "    importer = Ai4MarsImporter()\n",
    "    X, y, _ = importer(PATH=data_path, NUM_IMAGES=num_images, SAVE_PATH=save_path, SIZE=128)\n",
    "\n",
    "    transform = None\n",
    "    # Uncomment the following lines to apply transformations to the dataset\n",
    "    '''\n",
    "    transform = transforms.RandomChoice([\n",
    "     transforms.RandomRotation(90)])\n",
    "    '''\n",
    "\n",
    "    # Load info\n",
    "    load_info = './.info.pt'\n",
    "    info = torch.load(load_info)\n",
    "    \n",
    "    # Split the dataset\n",
    "    splitter = Ai4MarsSplitter()\n",
    "    train_set, test_set, val_set = splitter(X, y, [0.7, 0.2, 0.1], transform=transform,\n",
    "                                            SAVE_PATH=save_path)\n",
    "\n",
    "    # Build Ai4MarsDataloader\n",
    "    loader = Ai4MarsDataLoader()\n",
    "    train_loader, test_loader, val_loader = loader([train_set, test_set, val_set], [32, 16, 16],\n",
    "                                                   SAVE_PATH=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1689164179981,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "O3ytN-ugjHcW",
    "outputId": "29fe6dcc-1983-4c1b-a1e5-eabff10ea398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n  print(img.device)\\n  print(img.shape)\\n  if (img.device != 'cpu'):\\n    img.to('cpu')\\n    print('test')\\n  print(img.device)\\n  plt.imshow(img)\\n  plt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show_image(imgs):\n",
    "    if len(imgs.size()) == 4:\n",
    "        for img in imgs:\n",
    "            imgs = imgs.permute(2,0,1)\n",
    "    else:\n",
    "        imgs = imgs.permute(2,0,1)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "'''\n",
    "  print(img.device)\n",
    "  print(img.shape)\n",
    "  if (img.device != 'cpu'):\n",
    "    img.to('cpu')\n",
    "    print('test')\n",
    "  print(img.device)\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1689164179986,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "3lU21D6nXULX"
   },
   "outputs": [],
   "source": [
    "\"\"\" Trainer module for MER-Segmentation 2.0 \"\"\"\n",
    "test2 = False\n",
    "if test2:\n",
    "\n",
    "    import time\n",
    "    from datetime import datetime \n",
    "\n",
    "    # This class collects all the training functionalities to train different models\n",
    "    class Ai4MarsTrainer():\n",
    "\n",
    "        # Initialization of training parameters in the class constructor\n",
    "        def __init__(self, loss_fn, optimizer, train_loader, val_loader,\n",
    "                     transform=None, device='cpu', save_state=None):\n",
    "            self.loss_fn = loss_fn\n",
    "            self.optimizer = optimizer\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.device = device\n",
    "            self.save_state = save_state\n",
    "            self.transform = transform\n",
    "            self.loss_list = []\n",
    "            self.tloss_list = []\n",
    "\n",
    "        # This function implements training for just one epoch\n",
    "        def train_one_epoch(self, model, epoch_index=0):\n",
    "            running_loss = 0.\n",
    "            last_loss = 0.\n",
    "\n",
    "            # parameters for online data augmentation (batch transformation)\n",
    "            running_tloss = 0.\n",
    "            last_tloss = 0.\n",
    "            t_index = 0 \n",
    "\n",
    "            for batch_index, batch in enumerate(self.train_loader):\n",
    "                # Every data instance is an (input, label) pair\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Zero your gradients for every batch!\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Send inputs and labels to GPU (or whatever device is)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Adjust label to be 2D tensors of batch size\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels.long()\n",
    "\n",
    "                # Compute the loss and its gradients\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                # Adjust learning weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # if transformation exists apply them to the batch\n",
    "                if self.transform:\n",
    "                    tinputs = self.transform(inputs)\n",
    "                    tinputs = tinputs.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    toutputs = model(tinputs)\n",
    "                    tloss = self.loss_fn(toutputs, labels)\n",
    "                    tloss.backward()\n",
    "                    running_tloss = tloss.item()\n",
    "                    t_index += 1\n",
    "                    tinputs.detach()\n",
    "                    del tinputs\n",
    "\n",
    "                # Free up RAM/VRAM\n",
    "                inputs.detach()\n",
    "                labels.detach()\n",
    "                del inputs\n",
    "                del labels\n",
    "\n",
    "            # Compute the average loss over all batches\n",
    "            last_loss =  running_loss / (batch_index + 1)\n",
    "\n",
    "            # Print report at the end of the last batch\n",
    "            print(f'Epoch {epoch_index+1}')\n",
    "            print(f'LOSS ON TRAIN: {last_loss}')\n",
    "\n",
    "            if self.transform:\n",
    "                last_tloss = running_tloss / (t_index + 1)\n",
    "                print(f'LOSS ON TRANSFORMED-TRAIN: {last_tloss}')\n",
    "\n",
    "            else:\n",
    "                last_tloss = None\n",
    "\n",
    "            return last_loss, last_tloss\n",
    "\n",
    "        # This function implements training for multiple epochs\n",
    "        def train_multiple_epoch(self, model, EPOCHS=100):\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            epoch_number = 0 # just a counter\n",
    "            best_vloss = 1_000_000.\n",
    "            self.loss_list = []\n",
    "            self.tloss_list = []\n",
    "\n",
    "            for epoch in range(EPOCHS):\n",
    "                # Make sure gradient tracking is on, and do a pass over the data\n",
    "                model.train(True)\n",
    "\n",
    "                # Start monitoring training time\n",
    "                start = time.time()\n",
    "\n",
    "                avg_loss = self.train_one_epoch(model, epoch)\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                # We don't need gradients on to do reporting\n",
    "                model.train(False)\n",
    "\n",
    "                # Test loss\n",
    "                running_vloss = 0.0\n",
    "                for vbatch_index, vbatch in enumerate(self.val_loader):\n",
    "\n",
    "                    # Every data instance is a (input, label) pair\n",
    "                    vinputs, vlabels = vbatch\n",
    "\n",
    "                    # Send inputs and labels to GPU\n",
    "                    vinputs = vinputs.to(self.device)\n",
    "                    vlabels = vlabels.to(self.device)\n",
    "\n",
    "                    # Model prediction\n",
    "                    voutputs = model(vinputs)\n",
    "\n",
    "                    # Send inputs and labels to GPU (or whatever device is)\n",
    "                    vlabels = vlabels.squeeze()\n",
    "                    vlabels = vlabels.long()\n",
    "\n",
    "                    # Run validation loss\n",
    "                    val_loss = self.loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += val_loss.item()\n",
    "\n",
    "                    # Free up RAM/VRAM\n",
    "                    vinputs.detach()\n",
    "                    vlabels.detach()\n",
    "                    del vinputs\n",
    "                    del vlabels\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Compute the average loss over all batches\n",
    "                avg_vloss = running_vloss / (vbatch_index + 1)\n",
    "\n",
    "                print(\"Time needed for training: \" + str(end-start)+ \" seconds\")\n",
    "\n",
    "                # Print report at the end of the epoch\n",
    "                print(f'LOSS ON VALIDATION: {avg_vloss}')\n",
    "\n",
    "                # Save loss in a list to then perform metrics evaluation\n",
    "                self.loss_list.append((avg_loss[0], end-start))\n",
    "\n",
    "                # If online data augmentation has been performed:\n",
    "                if avg_loss[1]:\n",
    "                    self.tloss_list.append((avg_loss[1], end-start))\n",
    "\n",
    "                # Track best performance, and save the model's state\n",
    "                if avg_vloss < best_vloss:\n",
    "                    best_vloss = avg_vloss\n",
    "                    model_path = self.save_state + 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test2:\n",
    "    from typing import Optional\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    # based on:\n",
    "    # https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n",
    "\n",
    "    class DiceLoss(nn.Module):\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            super(DiceLoss, self).__init__()\n",
    "            self.eps: float = 1e-6\n",
    "\n",
    "        def forward(\n",
    "                self,\n",
    "                input: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\n",
    "            #errors:\n",
    "            if not torch.is_tensor(input):\n",
    "                raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                                .format(type(input)))\n",
    "            if not len(input.shape) == 4:\n",
    "                raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                                 .format(input.shape))\n",
    "            if not input.shape[-2:] == target.shape[-2:]:\n",
    "                raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                                 .format(input.shape, input.shape))\n",
    "            if not input.device == target.device:\n",
    "                raise ValueError(\n",
    "                    \"input and target must be in the same device. Got: {}\" .format(\n",
    "                        input.device, target.device))\n",
    "\n",
    "\n",
    "            num_classes = input.shape[1]\n",
    "\n",
    "            # compute softmax over the classes axis\n",
    "            input_soft = F.softmax(input, dim=1)\n",
    "\n",
    "            # print(\"shape\", input_soft.shape)\n",
    "\n",
    "            input_soft = input_soft.permute(0,2,1,3)\n",
    "            input_soft = input_soft.permute(0,1,3,2)\n",
    "\n",
    "            # create the labels one hot tensor\n",
    "            target_one_hot = F.one_hot(target, num_classes=input.shape[1])\n",
    "\n",
    "            # target_one_hot = target_one_hot.permute(0,1,3,2)\n",
    "            # target_one_hot = target_one_hot.permute(0,2,1,3)\n",
    "\n",
    "            # print(\"target_one_hot\",target_one_hot.shape)\n",
    "            # print(\"input_soft\",input_soft.shape)\n",
    "\n",
    "\n",
    "            # compute the actual dice score\n",
    "            dims = (1, 2, 3)\n",
    "            intersection = torch.sum(input_soft.reshape(-1) * target_one_hot.reshape(-1), -1)\n",
    "            cardinality = torch.sum(input_soft.reshape(-1) + target_one_hot.reshape(-1), -1)\n",
    "\n",
    "            dice_score = 2. * intersection / (cardinality + self.eps)\n",
    "\n",
    "            # if (self.stampa):\n",
    "            #   print(\"outputs\",input_soft)\n",
    "            #   print(\"labels\",target_one_hot)\n",
    "            #   print((1. - dice_score).item())\n",
    "\n",
    "\n",
    "\n",
    "            # print((1. - dice_score).shape)\n",
    "\n",
    "\n",
    "\n",
    "            return 1. - dice_score\n",
    "\n",
    "            #Alternative Universe\n",
    "\n",
    "            # labels = target_one_hot\n",
    "            # preds = input_soft\n",
    "\n",
    "            # tp = torch.sum(labels*preds, dim=(2, 3))\n",
    "            # fn = torch.sum(labels*(1-preds), dim=(2, 3))\n",
    "            # fp = torch.sum((1-labels)*preds, dim=(2, 3))\n",
    "\n",
    "\n",
    "            # delta = 0.5 # va messo come hyperparameter\n",
    "\n",
    "            # dice_score = (tp + 1e-6) / (tp + delta * fn + (1 - delta) * fp + 1e-6)\n",
    "            # dice_score = torch.sum(1 - dice_score, dim=-1)\n",
    "\n",
    "            # dice_score = dice_score / num_classes\n",
    "            # return dice_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVll-qRSXULY",
    "outputId": "e125181d-a822-4c69-f272-ea55ec43af18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train loss: 0.8007287708195773\n",
      "Time needed for training: 20.027595043182373 seconds\n",
      "Validation loss: 0.8013338148593903 \n",
      "\n",
      "EPOCH 2\n",
      "Train loss: 0.8014303608374163\n",
      "Time needed for training: 19.30750608444214 seconds\n",
      "Validation loss: 0.8059030324220657 \n",
      "\n",
      "EPOCH 3\n",
      "Train loss: 0.799629043449055\n",
      "Time needed for training: 19.458564519882202 seconds\n",
      "Validation loss: 0.7939244955778122 \n",
      "\n",
      "EPOCH 4\n",
      "Train loss: 0.7972212921489369\n",
      "Time needed for training: 19.17762303352356 seconds\n",
      "Validation loss: 0.8031769096851349 \n",
      "\n",
      "EPOCH 5\n",
      "Train loss: 0.8046101602641019\n",
      "Time needed for training: 19.353991746902466 seconds\n",
      "Validation loss: 0.7937463521957397 \n",
      "\n",
      "EPOCH 6\n",
      "Train loss: 0.8009483380751177\n",
      "Time needed for training: 19.203519821166992 seconds\n",
      "Validation loss: 0.7993349730968475 \n",
      "\n",
      "EPOCH 7\n",
      "Train loss: 0.801039684902538\n",
      "Time needed for training: 19.066221952438354 seconds\n",
      "Validation loss: 0.7939426600933075 \n",
      "\n",
      "EPOCH 8\n",
      "Train loss: 0.7947778376665983\n",
      "Time needed for training: 19.20322346687317 seconds\n",
      "Validation loss: 0.8001841902732849 \n",
      "\n",
      "EPOCH 9\n",
      "Train loss: 0.7983557256785306\n",
      "Time needed for training: 19.588574409484863 seconds\n",
      "Validation loss: 0.7955118119716644 \n",
      "\n",
      "EPOCH 10\n",
      "Train loss: 0.7997945113615557\n",
      "Time needed for training: 19.531087636947632 seconds\n",
      "Validation loss: 0.7934702038764954 \n",
      "\n",
      "EPOCH 11\n",
      "Train loss: 0.7999071207913485\n",
      "Time needed for training: 19.526921033859253 seconds\n",
      "Validation loss: 0.8041660636663437 \n",
      "\n",
      "EPOCH 12\n",
      "Train loss: 0.8002613241022284\n",
      "Time needed for training: 19.50632953643799 seconds\n",
      "Validation loss: 0.7989194095134735 \n",
      "\n",
      "EPOCH 13\n",
      "Train loss: 0.7979490215128119\n",
      "Time needed for training: 19.46368670463562 seconds\n",
      "Validation loss: 0.796667292714119 \n",
      "\n",
      "EPOCH 14\n",
      "Train loss: 0.7989455732432279\n",
      "Time needed for training: 19.56366753578186 seconds\n",
      "Validation loss: 0.7993342876434326 \n",
      "\n",
      "EPOCH 15\n",
      "Train loss: 0.7999975302002647\n",
      "Time needed for training: 19.547621488571167 seconds\n",
      "Validation loss: 0.7976746261119843 \n",
      "\n",
      "EPOCH 16\n",
      "Train loss: 0.79931302504106\n",
      "Time needed for training: 19.313209533691406 seconds\n",
      "Validation loss: 0.802526518702507 \n",
      "\n",
      "EPOCH 17\n",
      "Train loss: 0.8033873005346819\n",
      "Time needed for training: 19.478219985961914 seconds\n",
      "Validation loss: 0.7978154122829437 \n",
      "\n",
      "EPOCH 18\n",
      "Train loss: 0.8006237149238586\n",
      "Time needed for training: 19.4664249420166 seconds\n",
      "Validation loss: 0.7992048263549805 \n",
      "\n",
      "EPOCH 19\n",
      "Train loss: 0.8013848662376404\n",
      "Time needed for training: 19.237014532089233 seconds\n",
      "Validation loss: 0.802647203207016 \n",
      "\n",
      "EPOCH 20\n",
      "Train loss: 0.8011755076321688\n",
      "Time needed for training: 19.46204113960266 seconds\n",
      "Validation loss: 0.7949270308017731 \n",
      "\n",
      "EPOCH 21\n",
      "Train loss: 0.7986997582695701\n",
      "Time needed for training: 19.443244218826294 seconds\n",
      "Validation loss: 0.8076325058937073 \n",
      "\n",
      "EPOCH 22\n",
      "Train loss: 0.7999098300933838\n",
      "Time needed for training: 19.89621376991272 seconds\n",
      "Validation loss: 0.8019320070743561 \n",
      "\n",
      "EPOCH 23\n",
      "Train loss: 0.8002443530342795\n",
      "Time needed for training: 20.071088314056396 seconds\n",
      "Validation loss: 0.791998565196991 \n",
      "\n",
      "EPOCH 24\n",
      "Train loss: 0.7994014945897189\n",
      "Time needed for training: 20.099514484405518 seconds\n",
      "Validation loss: 0.8039229959249496 \n",
      "\n",
      "EPOCH 25\n",
      "Train loss: 0.8017308061773126\n",
      "Time needed for training: 20.04835557937622 seconds\n",
      "Validation loss: 0.8006878197193146 \n",
      "\n",
      "EPOCH 26\n",
      "Train loss: 0.7977630821141329\n",
      "Time needed for training: 19.76638174057007 seconds\n",
      "Validation loss: 0.7952588200569153 \n",
      "\n",
      "EPOCH 27\n",
      "Train loss: 0.7984368150884454\n",
      "Time needed for training: 20.101868867874146 seconds\n",
      "Validation loss: 0.8029040694236755 \n",
      "\n",
      "EPOCH 28\n",
      "Train loss: 0.8004220182245428\n",
      "Time needed for training: 20.596465349197388 seconds\n",
      "Validation loss: 0.795162707567215 \n",
      "\n",
      "EPOCH 29\n",
      "Train loss: 0.7974994670261036\n",
      "Time needed for training: 20.88719367980957 seconds\n",
      "Validation loss: 0.7935856282711029 \n",
      "\n",
      "EPOCH 30\n",
      "Train loss: 0.8025503700429742\n",
      "Time needed for training: 20.195561408996582 seconds\n",
      "Validation loss: 0.7945136576890945 \n",
      "\n",
      "EPOCH 31\n",
      "Train loss: 0.7975910793651234\n",
      "Time needed for training: 19.695157766342163 seconds\n",
      "Validation loss: 0.7995646297931671 \n",
      "\n",
      "EPOCH 32\n",
      "Train loss: 0.7986613674597307\n",
      "Time needed for training: 19.491929054260254 seconds\n",
      "Validation loss: 0.7977408617734909 \n",
      "\n",
      "EPOCH 33\n",
      "Train loss: 0.7968190745873884\n",
      "Time needed for training: 19.57241702079773 seconds\n",
      "Validation loss: 0.8001968860626221 \n",
      "\n",
      "EPOCH 34\n",
      "Train loss: 0.7974805452606895\n",
      "Time needed for training: 19.342780113220215 seconds\n",
      "Validation loss: 0.7939736247062683 \n",
      "\n",
      "EPOCH 35\n",
      "Train loss: 0.7997403849254955\n",
      "Time needed for training: 20.210253953933716 seconds\n",
      "Validation loss: 0.8049658536911011 \n",
      "\n",
      "EPOCH 36\n",
      "Train loss: 0.8000951734456149\n",
      "Time needed for training: 19.492189645767212 seconds\n",
      "Validation loss: 0.7955583930015564 \n",
      "\n",
      "EPOCH 37\n",
      "Train loss: 0.7974043488502502\n",
      "Time needed for training: 19.602684020996094 seconds\n",
      "Validation loss: 0.8014829009771347 \n",
      "\n",
      "EPOCH 38\n",
      "Train loss: 0.8002615950324319\n",
      "Time needed for training: 19.328405141830444 seconds\n",
      "Validation loss: 0.7994712889194489 \n",
      "\n",
      "EPOCH 39\n",
      "Train loss: 0.801074970852245\n",
      "Time needed for training: 19.752827405929565 seconds\n",
      "Validation loss: 0.8076328039169312 \n",
      "\n",
      "EPOCH 40\n",
      "Train loss: 0.8015401363372803\n",
      "Time needed for training: 19.689260482788086 seconds\n",
      "Validation loss: 0.7992362529039383 \n",
      "\n",
      "EPOCH 41\n",
      "Train loss: 0.8017841198227622\n",
      "Time needed for training: 19.725524187088013 seconds\n",
      "Validation loss: 0.7877447158098221 \n",
      "\n",
      "EPOCH 42\n",
      "Train loss: 0.8006614934314381\n",
      "Time needed for training: 19.61496329307556 seconds\n",
      "Validation loss: 0.8002744615077972 \n",
      "\n",
      "EPOCH 43\n",
      "Train loss: 0.7990794561126016\n",
      "Time needed for training: 19.822391748428345 seconds\n",
      "Validation loss: 0.7994291633367538 \n",
      "\n",
      "EPOCH 44\n",
      "Train loss: 0.8012604171579535\n",
      "Time needed for training: 19.701725721359253 seconds\n",
      "Validation loss: 0.7945092171430588 \n",
      "\n",
      "EPOCH 45\n",
      "Train loss: 0.7997347712516785\n",
      "Time needed for training: 19.80626654624939 seconds\n",
      "Validation loss: 0.8066109418869019 \n",
      "\n",
      "EPOCH 46\n",
      "Train loss: 0.7988828799941323\n",
      "Time needed for training: 19.78371548652649 seconds\n",
      "Validation loss: 0.8003330677747726 \n",
      "\n",
      "EPOCH 47\n",
      "Train loss: 0.8011235107075084\n",
      "Time needed for training: 19.647968292236328 seconds\n",
      "Validation loss: 0.8085935711860657 \n",
      "\n",
      "EPOCH 48\n",
      "Train loss: 0.7991705699400469\n",
      "Time needed for training: 19.690380811691284 seconds\n",
      "Validation loss: 0.8003697842359543 \n",
      "\n",
      "EPOCH 49\n",
      "Train loss: 0.8021164807406339\n",
      "Time needed for training: 19.855183362960815 seconds\n",
      "Validation loss: 0.7978327125310898 \n",
      "\n",
      "EPOCH 50\n",
      "Train loss: 0.8023613095283508\n",
      "Time needed for training: 19.841486930847168 seconds\n",
      "Validation loss: 0.8022009581327438 \n",
      "\n",
      "EPOCH 51\n",
      "Train loss: 0.8009474656798623\n",
      "Time needed for training: 19.93359875679016 seconds\n",
      "Validation loss: 0.7985111176967621 \n",
      "\n",
      "EPOCH 52\n",
      "Train loss: 0.8010312914848328\n",
      "Time needed for training: 19.659098863601685 seconds\n",
      "Validation loss: 0.800958514213562 \n",
      "\n",
      "EPOCH 53\n",
      "Train loss: 0.8007437911900607\n",
      "Time needed for training: 19.811951875686646 seconds\n",
      "Validation loss: 0.7975407093763351 \n",
      "\n",
      "EPOCH 54\n",
      "Train loss: 0.7998307293111627\n",
      "Time needed for training: 19.8058922290802 seconds\n",
      "Validation loss: 0.7990962564945221 \n",
      "\n",
      "EPOCH 55\n",
      "Train loss: 0.7988458492539146\n",
      "Time needed for training: 19.85400152206421 seconds\n",
      "Validation loss: 0.8062316477298737 \n",
      "\n",
      "EPOCH 56\n",
      "Train loss: 0.7971556945280596\n",
      "Time needed for training: 19.755546808242798 seconds\n",
      "Validation loss: 0.8016730844974518 \n",
      "\n",
      "EPOCH 57\n",
      "Train loss: 0.7972039742903276\n",
      "Time needed for training: 19.794872760772705 seconds\n",
      "Validation loss: 0.7979017049074173 \n",
      "\n",
      "EPOCH 58\n",
      "Train loss: 0.7964893200180747\n",
      "Time needed for training: 19.769781351089478 seconds\n",
      "Validation loss: 0.7968057841062546 \n",
      "\n",
      "EPOCH 59\n",
      "Train loss: 0.798737184567885\n",
      "Time needed for training: 19.930092811584473 seconds\n",
      "Validation loss: 0.7997934371232986 \n",
      "\n",
      "EPOCH 60\n",
      "Train loss: 0.8042967590418729\n",
      "Time needed for training: 19.83563780784607 seconds\n",
      "Validation loss: 0.7958908528089523 \n",
      "\n",
      "EPOCH 61\n",
      "Train loss: 0.798796225677837\n",
      "Time needed for training: 19.945344924926758 seconds\n",
      "Validation loss: 0.7996210157871246 \n",
      "\n",
      "EPOCH 62\n",
      "Train loss: 0.8016787333921953\n",
      "Time needed for training: 19.80531644821167 seconds\n",
      "Validation loss: 0.8015461266040802 \n",
      "\n",
      "EPOCH 63\n",
      "Train loss: 0.8014010461893949\n",
      "Time needed for training: 19.312211275100708 seconds\n",
      "Validation loss: 0.8048940151929855 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 64\n",
      "Train loss: 0.8007281856103377\n",
      "Time needed for training: 19.44651699066162 seconds\n",
      "Validation loss: 0.8003478050231934 \n",
      "\n",
      "EPOCH 65\n",
      "Train loss: 0.8037519996816461\n",
      "Time needed for training: 19.01421022415161 seconds\n",
      "Validation loss: 0.8002745360136032 \n",
      "\n",
      "EPOCH 66\n",
      "Train loss: 0.7980618476867676\n",
      "Time needed for training: 19.76322078704834 seconds\n",
      "Validation loss: 0.8001479357481003 \n",
      "\n",
      "EPOCH 67\n",
      "Train loss: 0.8000954660502347\n",
      "Time needed for training: 19.54409384727478 seconds\n",
      "Validation loss: 0.794793501496315 \n",
      "\n",
      "EPOCH 68\n",
      "Train loss: 0.7988112297925082\n",
      "Time needed for training: 19.278228998184204 seconds\n",
      "Validation loss: 0.8017802536487579 \n",
      "\n",
      "EPOCH 69\n",
      "Train loss: 0.7976743524724786\n",
      "Time needed for training: 19.377512454986572 seconds\n",
      "Validation loss: 0.8035624921321869 \n",
      "\n",
      "EPOCH 70\n",
      "Train loss: 0.8022956143726002\n",
      "Time needed for training: 19.40351366996765 seconds\n",
      "Validation loss: 0.8004107773303986 \n",
      "\n",
      "EPOCH 71\n",
      "Train loss: 0.8004947629841891\n",
      "Time needed for training: 19.52165699005127 seconds\n",
      "Validation loss: 0.8015002608299255 \n",
      "\n",
      "EPOCH 72\n",
      "Train loss: 0.7981319698420438\n",
      "Time needed for training: 19.561259269714355 seconds\n",
      "Validation loss: 0.8003363162279129 \n",
      "\n",
      "EPOCH 73\n",
      "Train loss: 0.8022818782112815\n",
      "Time needed for training: 19.441460609436035 seconds\n",
      "Validation loss: 0.8061417639255524 \n",
      "\n",
      "EPOCH 74\n",
      "Train loss: 0.7999199520457875\n",
      "Time needed for training: 19.149596691131592 seconds\n",
      "Validation loss: 0.8005535453557968 \n",
      "\n",
      "EPOCH 75\n",
      "Train loss: 0.7975596622987227\n",
      "Time needed for training: 19.109784364700317 seconds\n",
      "Validation loss: 0.7981114685535431 \n",
      "\n",
      "EPOCH 76\n",
      "Train loss: 0.8018829714168202\n",
      "Time needed for training: 19.573223114013672 seconds\n",
      "Validation loss: 0.8064063191413879 \n",
      "\n",
      "EPOCH 77\n",
      "Train loss: 0.7998714501207526\n",
      "Time needed for training: 19.425275325775146 seconds\n",
      "Validation loss: 0.8079891949892044 \n",
      "\n",
      "EPOCH 78\n",
      "Train loss: 0.8032715754075483\n",
      "Time needed for training: 19.164753913879395 seconds\n",
      "Validation loss: 0.7990511208772659 \n",
      "\n",
      "EPOCH 79\n",
      "Train loss: 0.7996947927908464\n",
      "Time needed for training: 18.959975719451904 seconds\n",
      "Validation loss: 0.7994717806577682 \n",
      "\n",
      "EPOCH 80\n",
      "Train loss: 0.8008933609182184\n",
      "Time needed for training: 19.070140838623047 seconds\n",
      "Validation loss: 0.7970021814107895 \n",
      "\n",
      "EPOCH 81\n",
      "Train loss: 0.7983916564421221\n",
      "Time needed for training: 19.51866388320923 seconds\n",
      "Validation loss: 0.7982078939676285 \n",
      "\n",
      "EPOCH 82\n",
      "Train loss: 0.7999326207421043\n",
      "Time needed for training: 19.59122610092163 seconds\n",
      "Validation loss: 0.7979692816734314 \n",
      "\n",
      "EPOCH 83\n",
      "Train loss: 0.7982841838489879\n",
      "Time needed for training: 19.66578197479248 seconds\n",
      "Validation loss: 0.8029162287712097 \n",
      "\n",
      "EPOCH 84\n",
      "Train loss: 0.8025156909769232\n",
      "Time needed for training: 19.355932235717773 seconds\n",
      "Validation loss: 0.8024988025426865 \n",
      "\n",
      "EPOCH 85\n",
      "Train loss: 0.8000114018266852\n",
      "Time needed for training: 19.269527912139893 seconds\n",
      "Validation loss: 0.7987762093544006 \n",
      "\n",
      "EPOCH 86\n",
      "Train loss: 0.8041825565424833\n",
      "Time needed for training: 19.321800708770752 seconds\n",
      "Validation loss: 0.8058488965034485 \n",
      "\n",
      "EPOCH 87\n",
      "Train loss: 0.7980004007166083\n",
      "Time needed for training: 19.271722555160522 seconds\n",
      "Validation loss: 0.8016322106122971 \n",
      "\n",
      "EPOCH 88\n",
      "Train loss: 0.7979296445846558\n",
      "Time needed for training: 19.608530521392822 seconds\n",
      "Validation loss: 0.7992578446865082 \n",
      "\n",
      "EPOCH 89\n",
      "Train loss: 0.8035146268931302\n",
      "Time needed for training: 19.63419198989868 seconds\n",
      "Validation loss: 0.8056938499212265 \n",
      "\n",
      "EPOCH 90\n",
      "Train loss: 0.7981672232801263\n",
      "Time needed for training: 19.756612539291382 seconds\n",
      "Validation loss: 0.8006396591663361 \n",
      "\n",
      "EPOCH 91\n",
      "Train loss: 0.8000885844230652\n",
      "Time needed for training: 19.640888690948486 seconds\n",
      "Validation loss: 0.8017792701721191 \n",
      "\n",
      "EPOCH 92\n",
      "Train loss: 0.7987896366552873\n",
      "Time needed for training: 19.344931602478027 seconds\n",
      "Validation loss: 0.7999077141284943 \n",
      "\n",
      "EPOCH 93\n",
      "Train loss: 0.8003178834915161\n",
      "Time needed for training: 19.404840230941772 seconds\n",
      "Validation loss: 0.7974002063274384 \n",
      "\n",
      "EPOCH 94\n",
      "Train loss: 0.7990227395837958\n",
      "Time needed for training: 19.429988622665405 seconds\n",
      "Validation loss: 0.8009709119796753 \n",
      "\n",
      "EPOCH 95\n",
      "Train loss: 0.8012897263873707\n",
      "Time needed for training: 19.27674627304077 seconds\n",
      "Validation loss: 0.79808309674263 \n",
      "\n",
      "EPOCH 96\n",
      "Train loss: 0.7999257120219144\n",
      "Time needed for training: 19.520598649978638 seconds\n",
      "Validation loss: 0.7996064573526382 \n",
      "\n",
      "EPOCH 97\n",
      "Train loss: 0.8013665188442577\n",
      "Time needed for training: 19.31860589981079 seconds\n",
      "Validation loss: 0.8019289672374725 \n",
      "\n",
      "EPOCH 98\n",
      "Train loss: 0.7988216551867399\n",
      "Time needed for training: 19.585262537002563 seconds\n",
      "Validation loss: 0.7987218648195267 \n",
      "\n",
      "EPOCH 99\n",
      "Train loss: 0.8006990226832303\n",
      "Time needed for training: 19.229933738708496 seconds\n",
      "Validation loss: 0.7960964739322662 \n",
      "\n",
      "EPOCH 100\n",
      "Train loss: 0.8020192818208174\n",
      "Time needed for training: 19.36665391921997 seconds\n",
      "Validation loss: 0.7976149022579193 \n",
      "\n",
      "EPOCH 101\n",
      "Train loss: 0.7998131513595581\n",
      "Time needed for training: 19.688891410827637 seconds\n",
      "Validation loss: 0.8026125431060791 \n",
      "\n",
      "EPOCH 102\n",
      "Train loss: 0.796361657706174\n",
      "Time needed for training: 19.328758239746094 seconds\n",
      "Validation loss: 0.8010321706533432 \n",
      "\n",
      "EPOCH 103\n",
      "Train loss: 0.8003013784235175\n",
      "Time needed for training: 19.398126125335693 seconds\n",
      "Validation loss: 0.7992082387208939 \n",
      "\n",
      "EPOCH 104\n",
      "Train loss: 0.7951496243476868\n",
      "Time needed for training: 19.351468563079834 seconds\n",
      "Validation loss: 0.8050099611282349 \n",
      "\n",
      "EPOCH 105\n",
      "Train loss: 0.7997628396207636\n",
      "Time needed for training: 19.651823043823242 seconds\n",
      "Validation loss: 0.797494500875473 \n",
      "\n",
      "EPOCH 106\n",
      "Train loss: 0.7995198639956388\n",
      "Time needed for training: 19.697903156280518 seconds\n",
      "Validation loss: 0.7995195686817169 \n",
      "\n",
      "EPOCH 107\n",
      "Train loss: 0.7974772507494147\n",
      "Time needed for training: 18.876349210739136 seconds\n",
      "Validation loss: 0.806282177567482 \n",
      "\n",
      "EPOCH 108\n",
      "Train loss: 0.7981950153004039\n",
      "Time needed for training: 19.21800398826599 seconds\n",
      "Validation loss: 0.8048509508371353 \n",
      "\n",
      "EPOCH 109\n",
      "Train loss: 0.8003373742103577\n",
      "Time needed for training: 19.725449323654175 seconds\n",
      "Validation loss: 0.7969058007001877 \n",
      "\n",
      "EPOCH 110\n",
      "Train loss: 0.8032465197823264\n",
      "Time needed for training: 19.294193744659424 seconds\n",
      "Validation loss: 0.8008914291858673 \n",
      "\n",
      "EPOCH 111\n",
      "Train loss: 0.8002016706900164\n",
      "Time needed for training: 19.282058477401733 seconds\n",
      "Validation loss: 0.8069948554039001 \n",
      "\n",
      "EPOCH 112\n",
      "Train loss: 0.7997828938744285\n",
      "Time needed for training: 19.36484670639038 seconds\n",
      "Validation loss: 0.7955235838890076 \n",
      "\n",
      "EPOCH 113\n",
      "Train loss: 0.7999544035304677\n",
      "Time needed for training: 19.519837617874146 seconds\n",
      "Validation loss: 0.7988002896308899 \n",
      "\n",
      "EPOCH 114\n",
      "Train loss: 0.7990535497665405\n",
      "Time needed for training: 19.578319787979126 seconds\n",
      "Validation loss: 0.7973494380712509 \n",
      "\n",
      "EPOCH 115\n",
      "Train loss: 0.7989209511063315\n",
      "Time needed for training: 19.369065284729004 seconds\n",
      "Validation loss: 0.7995118349790573 \n",
      "\n",
      "EPOCH 116\n",
      "Train loss: 0.7985367720777338\n",
      "Time needed for training: 19.60665512084961 seconds\n",
      "Validation loss: 0.8035450279712677 \n",
      "\n",
      "EPOCH 117\n",
      "Train loss: 0.7995584282008085\n",
      "Time needed for training: 19.594225883483887 seconds\n",
      "Validation loss: 0.7971220165491104 \n",
      "\n",
      "EPOCH 118\n",
      "Train loss: 0.7993061109022661\n",
      "Time needed for training: 19.841811418533325 seconds\n",
      "Validation loss: 0.8043560087680817 \n",
      "\n",
      "EPOCH 119\n",
      "Train loss: 0.7987628795883872\n",
      "Time needed for training: 19.543758630752563 seconds\n",
      "Validation loss: 0.8022400289773941 \n",
      "\n",
      "EPOCH 120\n",
      "Train loss: 0.8001706166700884\n",
      "Time needed for training: 19.44926428794861 seconds\n",
      "Validation loss: 0.8010298311710358 \n",
      "\n",
      "EPOCH 121\n",
      "Train loss: 0.8020000457763672\n",
      "Time needed for training: 19.426405906677246 seconds\n",
      "Validation loss: 0.7942216545343399 \n",
      "\n",
      "EPOCH 122\n",
      "Train loss: 0.8025376309048046\n",
      "Time needed for training: 19.20952606201172 seconds\n",
      "Validation loss: 0.7924972772598267 \n",
      "\n",
      "EPOCH 123\n",
      "Train loss: 0.7980222322724082\n",
      "Time needed for training: 19.414993047714233 seconds\n",
      "Validation loss: 0.7993168234825134 \n",
      "\n",
      "EPOCH 124\n",
      "Train loss: 0.7970666126771406\n",
      "Time needed for training: 19.606533527374268 seconds\n",
      "Validation loss: 0.8030057549476624 \n",
      "\n",
      "EPOCH 125\n",
      "Train loss: 0.8016017133539374\n",
      "Time needed for training: 19.843374967575073 seconds\n",
      "Validation loss: 0.7946236580610275 \n",
      "\n",
      "EPOCH 126\n",
      "Train loss: 0.8011763854460283\n",
      "Time needed for training: 19.419199228286743 seconds\n",
      "Validation loss: 0.7916467040777206 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 127\n",
      "Train loss: 0.8003386746753346\n",
      "Time needed for training: 19.37160325050354 seconds\n",
      "Validation loss: 0.7982690036296844 \n",
      "\n",
      "EPOCH 128\n",
      "Train loss: 0.7986401319503784\n",
      "Time needed for training: 19.30334162712097 seconds\n",
      "Validation loss: 0.7983914464712143 \n",
      "\n",
      "EPOCH 129\n",
      "Train loss: 0.8000447370789268\n",
      "Time needed for training: 19.42056894302368 seconds\n",
      "Validation loss: 0.7992695868015289 \n",
      "\n",
      "EPOCH 130\n",
      "Train loss: 0.8005446845834906\n",
      "Time needed for training: 19.800422430038452 seconds\n",
      "Validation loss: 0.7982134073972702 \n",
      "\n",
      "EPOCH 131\n",
      "Train loss: 0.8012941913171248\n",
      "Time needed for training: 19.622004747390747 seconds\n",
      "Validation loss: 0.7994853854179382 \n",
      "\n",
      "EPOCH 132\n",
      "Train loss: 0.7991696487773549\n",
      "Time needed for training: 20.398386240005493 seconds\n",
      "Validation loss: 0.7999256104230881 \n",
      "\n",
      "EPOCH 133\n",
      "Train loss: 0.7982811873609369\n",
      "Time needed for training: 19.61693835258484 seconds\n",
      "Validation loss: 0.7981073707342148 \n",
      "\n",
      "EPOCH 134\n",
      "Train loss: 0.802237256006761\n",
      "Time needed for training: 19.408924341201782 seconds\n",
      "Validation loss: 0.8021981418132782 \n",
      "\n",
      "EPOCH 135\n",
      "Train loss: 0.7998586242849176\n",
      "Time needed for training: 19.79871702194214 seconds\n",
      "Validation loss: 0.7994345128536224 \n",
      "\n",
      "EPOCH 136\n",
      "Train loss: 0.7999094887213274\n",
      "Time needed for training: 19.32282519340515 seconds\n",
      "Validation loss: 0.800348773598671 \n",
      "\n",
      "EPOCH 137\n",
      "Train loss: 0.7992890856482766\n",
      "Time needed for training: 20.540472269058228 seconds\n",
      "Validation loss: 0.7990903258323669 \n",
      "\n",
      "EPOCH 138\n",
      "Train loss: 0.8000299659642306\n",
      "Time needed for training: 21.587767124176025 seconds\n",
      "Validation loss: 0.8045322895050049 \n",
      "\n",
      "EPOCH 139\n",
      "Train loss: 0.8012269897894426\n",
      "Time needed for training: 20.418002128601074 seconds\n",
      "Validation loss: 0.8042988628149033 \n",
      "\n",
      "EPOCH 140\n",
      "Train loss: 0.7992566715587269\n",
      "Time needed for training: 20.752004623413086 seconds\n",
      "Validation loss: 0.7962747514247894 \n",
      "\n",
      "EPOCH 141\n",
      "Train loss: 0.7983635284683921\n",
      "Time needed for training: 19.70763373374939 seconds\n",
      "Validation loss: 0.7993247956037521 \n",
      "\n",
      "EPOCH 142\n",
      "Train loss: 0.7982228181578896\n",
      "Time needed for training: 20.014422178268433 seconds\n",
      "Validation loss: 0.8026115596294403 \n",
      "\n",
      "EPOCH 143\n",
      "Train loss: 0.8021469874815508\n",
      "Time needed for training: 19.60617423057556 seconds\n",
      "Validation loss: 0.7968934178352356 \n",
      "\n",
      "EPOCH 144\n",
      "Train loss: 0.8002886609597639\n",
      "Time needed for training: 19.431937217712402 seconds\n",
      "Validation loss: 0.8009087294340134 \n",
      "\n",
      "EPOCH 145\n",
      "Train loss: 0.8016276088627902\n",
      "Time needed for training: 19.32188057899475 seconds\n",
      "Validation loss: 0.8046620339155197 \n",
      "\n",
      "EPOCH 146\n",
      "Train loss: 0.7971897016872059\n",
      "Time needed for training: 19.257187366485596 seconds\n",
      "Validation loss: 0.8035430163145065 \n",
      "\n",
      "EPOCH 147\n",
      "Train loss: 0.79945815151388\n",
      "Time needed for training: 19.231369018554688 seconds\n",
      "Validation loss: 0.8011088669300079 \n",
      "\n",
      "EPOCH 148\n",
      "Train loss: 0.800317336212505\n",
      "Time needed for training: 19.121748447418213 seconds\n",
      "Validation loss: 0.7914629727602005 \n",
      "\n",
      "EPOCH 149\n",
      "Train loss: 0.7995247353206981\n",
      "Time needed for training: 19.007500648498535 seconds\n",
      "Validation loss: 0.8012497276067734 \n",
      "\n",
      "EPOCH 150\n",
      "Train loss: 0.7976351163604043\n",
      "Time needed for training: 19.23575210571289 seconds\n",
      "Validation loss: 0.7951367348432541 \n",
      "\n",
      "EPOCH 151\n",
      "Train loss: 0.7999264218590476\n",
      "Time needed for training: 19.206039905548096 seconds\n",
      "Validation loss: 0.8038302809000015 \n",
      "\n",
      "EPOCH 152\n",
      "Train loss: 0.7994659380479292\n",
      "Time needed for training: 19.589022397994995 seconds\n",
      "Validation loss: 0.8025933504104614 \n",
      "\n",
      "EPOCH 153\n",
      "Train loss: 0.7962303269993175\n",
      "Time needed for training: 19.43636393547058 seconds\n",
      "Validation loss: 0.8043922185897827 \n",
      "\n",
      "EPOCH 154\n",
      "Train loss: 0.7999438697641547\n",
      "Time needed for training: 19.42778706550598 seconds\n",
      "Validation loss: 0.8023293316364288 \n",
      "\n",
      "EPOCH 155\n",
      "Train loss: 0.8022144436836243\n",
      "Time needed for training: 19.222076654434204 seconds\n",
      "Validation loss: 0.8008023351430893 \n",
      "\n",
      "EPOCH 156\n",
      "Train loss: 0.7994074008681558\n",
      "Time needed for training: 19.376017808914185 seconds\n",
      "Validation loss: 0.7914616614580154 \n",
      "\n",
      "EPOCH 157\n",
      "Train loss: 0.7967407486655496\n",
      "Time needed for training: 19.464340209960938 seconds\n",
      "Validation loss: 0.8035909682512283 \n",
      "\n",
      "EPOCH 158\n",
      "Train loss: 0.7992681264877319\n",
      "Time needed for training: 19.117729902267456 seconds\n",
      "Validation loss: 0.8056654036045074 \n",
      "\n",
      "EPOCH 159\n",
      "Train loss: 0.8001230684193698\n",
      "Time needed for training: 19.109297275543213 seconds\n",
      "Validation loss: 0.803630605340004 \n",
      "\n",
      "EPOCH 160\n",
      "Train loss: 0.8022362373091958\n",
      "Time needed for training: 19.271902322769165 seconds\n",
      "Validation loss: 0.8027024567127228 \n",
      "\n",
      "EPOCH 161\n",
      "Train loss: 0.7996491627259688\n",
      "Time needed for training: 20.029060125350952 seconds\n",
      "Validation loss: 0.7979335635900497 \n",
      "\n",
      "EPOCH 162\n",
      "Train loss: 0.7967387221076272\n",
      "Time needed for training: 19.935901165008545 seconds\n",
      "Validation loss: 0.8023276925086975 \n",
      "\n",
      "EPOCH 163\n",
      "Train loss: 0.8028874343091791\n",
      "Time needed for training: 19.740461587905884 seconds\n",
      "Validation loss: 0.797548457980156 \n",
      "\n",
      "EPOCH 164\n",
      "Train loss: 0.79833139072765\n",
      "Time needed for training: 19.331395387649536 seconds\n",
      "Validation loss: 0.8040475249290466 \n",
      "\n",
      "EPOCH 165\n",
      "Train loss: 0.799867803400213\n",
      "Time needed for training: 19.678272485733032 seconds\n",
      "Validation loss: 0.8034673780202866 \n",
      "\n",
      "EPOCH 166\n",
      "Train loss: 0.7976769750768488\n",
      "Time needed for training: 19.96757745742798 seconds\n",
      "Validation loss: 0.8018248230218887 \n",
      "\n",
      "EPOCH 167\n",
      "Train loss: 0.8001373735341158\n",
      "Time needed for training: 20.422945261001587 seconds\n",
      "Validation loss: 0.7971005439758301 \n",
      "\n",
      "EPOCH 168\n",
      "Train loss: 0.7979095469821583\n",
      "Time needed for training: 20.36413812637329 seconds\n",
      "Validation loss: 0.8026508390903473 \n",
      "\n",
      "EPOCH 169\n",
      "Train loss: 0.8026229034770619\n",
      "Time needed for training: 19.976711988449097 seconds\n",
      "Validation loss: 0.7974335998296738 \n",
      "\n",
      "EPOCH 170\n",
      "Train loss: 0.8007791150699962\n",
      "Time needed for training: 20.068742990493774 seconds\n",
      "Validation loss: 0.8024494051933289 \n",
      "\n",
      "EPOCH 171\n",
      "Train loss: 0.7997911193154075\n",
      "Time needed for training: 20.214306116104126 seconds\n",
      "Validation loss: 0.7932920604944229 \n",
      "\n",
      "EPOCH 172\n",
      "Train loss: 0.8020336086099799\n",
      "Time needed for training: 20.413746118545532 seconds\n",
      "Validation loss: 0.8035368323326111 \n",
      "\n",
      "EPOCH 173\n",
      "Train loss: 0.7992751219055869\n",
      "Time needed for training: 20.72292923927307 seconds\n",
      "Validation loss: 0.8015803694725037 \n",
      "\n",
      "EPOCH 174\n",
      "Train loss: 0.8015162457119335\n",
      "Time needed for training: 20.8635995388031 seconds\n",
      "Validation loss: 0.8055597990751266 \n",
      "\n",
      "EPOCH 175\n",
      "Train loss: 0.8012316552075472\n",
      "Time needed for training: 20.90353012084961 seconds\n",
      "Validation loss: 0.7971094101667404 \n",
      "\n",
      "EPOCH 176\n",
      "Train loss: 0.7996789867227728\n",
      "Time needed for training: 21.440712213516235 seconds\n",
      "Validation loss: 0.8030775487422943 \n",
      "\n",
      "EPOCH 177\n",
      "Train loss: 0.7983289577744224\n",
      "Time needed for training: 21.794315338134766 seconds\n",
      "Validation loss: 0.8076103329658508 \n",
      "\n",
      "EPOCH 178\n",
      "Train loss: 0.7990548177198931\n",
      "Time needed for training: 21.245627403259277 seconds\n",
      "Validation loss: 0.7975237518548965 \n",
      "\n",
      "EPOCH 179\n",
      "Train loss: 0.7999991395256736\n",
      "Time needed for training: 20.672701597213745 seconds\n",
      "Validation loss: 0.7975023835897446 \n",
      "\n",
      "EPOCH 180\n",
      "Train loss: 0.7992337346076965\n",
      "Time needed for training: 20.350680351257324 seconds\n",
      "Validation loss: 0.8014196455478668 \n",
      "\n",
      "EPOCH 181\n",
      "Train loss: 0.7996640747243707\n",
      "Time needed for training: 19.885761260986328 seconds\n",
      "Validation loss: 0.8037301152944565 \n",
      "\n",
      "EPOCH 182\n",
      "Train loss: 0.7999127073721453\n",
      "Time needed for training: 19.559532165527344 seconds\n",
      "Validation loss: 0.8036628812551498 \n",
      "\n",
      "EPOCH 183\n",
      "Train loss: 0.7975407838821411\n",
      "Time needed for training: 19.473495721817017 seconds\n",
      "Validation loss: 0.8023568838834763 \n",
      "\n",
      "EPOCH 184\n",
      "Train loss: 0.79975656487725\n",
      "Time needed for training: 19.611012935638428 seconds\n",
      "Validation loss: 0.7982833087444305 \n",
      "\n",
      "EPOCH 185\n",
      "Train loss: 0.7994073575193231\n",
      "Time needed for training: 19.895745992660522 seconds\n",
      "Validation loss: 0.8037253320217133 \n",
      "\n",
      "EPOCH 186\n",
      "Train loss: 0.8030285889452154\n",
      "Time needed for training: 19.49762487411499 seconds\n",
      "Validation loss: 0.8010586649179459 \n",
      "\n",
      "EPOCH 187\n",
      "Train loss: 0.7980759902433916\n",
      "Time needed for training: 19.725080728530884 seconds\n",
      "Validation loss: 0.7916254103183746 \n",
      "\n",
      "EPOCH 188\n",
      "Train loss: 0.8009486306797374\n",
      "Time needed for training: 19.945780038833618 seconds\n",
      "Validation loss: 0.7986929565668106 \n",
      "\n",
      "EPOCH 189\n",
      "Train loss: 0.7978577613830566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed for training: 19.657479763031006 seconds\n",
      "Validation loss: 0.7933532297611237 \n",
      "\n",
      "EPOCH 190\n",
      "Train loss: 0.7985007491978732\n",
      "Time needed for training: 19.835646152496338 seconds\n",
      "Validation loss: 0.8020694851875305 \n",
      "\n",
      "EPOCH 191\n",
      "Train loss: 0.8014441620219838\n",
      "Time needed for training: 19.676208972930908 seconds\n",
      "Validation loss: 0.7990280240774155 \n",
      "\n",
      "EPOCH 192\n",
      "Train loss: 0.8006691282445734\n",
      "Time needed for training: 19.662424325942993 seconds\n",
      "Validation loss: 0.798660159111023 \n",
      "\n",
      "EPOCH 193\n",
      "Train loss: 0.7974587787281383\n",
      "Time needed for training: 19.55397319793701 seconds\n",
      "Validation loss: 0.7959902286529541 \n",
      "\n",
      "EPOCH 194\n",
      "Train loss: 0.7979286963289435\n",
      "Time needed for training: 19.57575297355652 seconds\n",
      "Validation loss: 0.798705667257309 \n",
      "\n",
      "EPOCH 195\n",
      "Train loss: 0.7986262278123335\n",
      "Time needed for training: 19.589356184005737 seconds\n",
      "Validation loss: 0.7983179986476898 \n",
      "\n",
      "EPOCH 196\n",
      "Train loss: 0.8005527962337841\n",
      "Time needed for training: 19.680185556411743 seconds\n",
      "Validation loss: 0.801029846072197 \n",
      "\n",
      "EPOCH 197\n",
      "Train loss: 0.8009968996047974\n",
      "Time needed for training: 19.778136491775513 seconds\n",
      "Validation loss: 0.799937903881073 \n",
      "\n",
      "EPOCH 198\n",
      "Train loss: 0.796706654808738\n",
      "Time needed for training: 19.665133714675903 seconds\n",
      "Validation loss: 0.7992831319570541 \n",
      "\n",
      "EPOCH 199\n",
      "Train loss: 0.8001807061108676\n",
      "Time needed for training: 19.45551609992981 seconds\n",
      "Validation loss: 0.8038860857486725 \n",
      "\n",
      "EPOCH 200\n",
      "Train loss: 0.803207895972512\n",
      "Time needed for training: 19.525749444961548 seconds\n",
      "Validation loss: 0.8037586361169815 \n",
      "\n",
      "EPOCH 201\n",
      "Train loss: 0.7972187887538563\n",
      "Time needed for training: 19.597076654434204 seconds\n",
      "Validation loss: 0.8054139614105225 \n",
      "\n",
      "EPOCH 202\n",
      "Train loss: 0.8021789193153381\n",
      "Time needed for training: 20.05524206161499 seconds\n",
      "Validation loss: 0.7943345457315445 \n",
      "\n",
      "EPOCH 203\n",
      "Train loss: 0.7966402714902704\n",
      "Time needed for training: 20.204763412475586 seconds\n",
      "Validation loss: 0.799139991402626 \n",
      "\n",
      "EPOCH 204\n",
      "Train loss: 0.7990787083452399\n",
      "Time needed for training: 19.969002962112427 seconds\n",
      "Validation loss: 0.8034290820360184 \n",
      "\n",
      "EPOCH 205\n",
      "Train loss: 0.8005955490199003\n",
      "Time needed for training: 19.605719089508057 seconds\n",
      "Validation loss: 0.797874316573143 \n",
      "\n",
      "EPOCH 206\n",
      "Train loss: 0.7987597638910467\n",
      "Time needed for training: 19.62195611000061 seconds\n",
      "Validation loss: 0.801826149225235 \n",
      "\n",
      "EPOCH 207\n",
      "Train loss: 0.8016203533519398\n",
      "Time needed for training: 19.49640202522278 seconds\n",
      "Validation loss: 0.8038443028926849 \n",
      "\n",
      "EPOCH 208\n",
      "Train loss: 0.8000422824512828\n",
      "Time needed for training: 19.775285482406616 seconds\n",
      "Validation loss: 0.8067655861377716 \n",
      "\n",
      "EPOCH 209\n",
      "Train loss: 0.8010642636906017\n",
      "Time needed for training: 19.58735752105713 seconds\n",
      "Validation loss: 0.8148103952407837 \n",
      "\n",
      "EPOCH 210\n",
      "Train loss: 0.801121478730982\n",
      "Time needed for training: 19.45272946357727 seconds\n",
      "Validation loss: 0.7974782884120941 \n",
      "\n",
      "EPOCH 211\n",
      "Train loss: 0.8011177778244019\n",
      "Time needed for training: 19.402884483337402 seconds\n",
      "Validation loss: 0.797075942158699 \n",
      "\n",
      "EPOCH 212\n",
      "Train loss: 0.7963437492197211\n",
      "Time needed for training: 19.49295997619629 seconds\n",
      "Validation loss: 0.7987972944974899 \n",
      "\n",
      "EPOCH 213\n",
      "Train loss: 0.7999922091310675\n",
      "Time needed for training: 19.544022798538208 seconds\n",
      "Validation loss: 0.8014661073684692 \n",
      "\n",
      "EPOCH 214\n",
      "Train loss: 0.801985117522153\n",
      "Time needed for training: 19.982690811157227 seconds\n",
      "Validation loss: 0.7960796803236008 \n",
      "\n",
      "EPOCH 215\n",
      "Train loss: 0.7949899326671254\n",
      "Time needed for training: 19.866796731948853 seconds\n",
      "Validation loss: 0.8005416542291641 \n",
      "\n",
      "EPOCH 216\n",
      "Train loss: 0.7983812689781189\n",
      "Time needed for training: 20.014275550842285 seconds\n",
      "Validation loss: 0.8031837791204453 \n",
      "\n",
      "EPOCH 217\n",
      "Train loss: 0.7936769940636375\n",
      "Time needed for training: 19.657628774642944 seconds\n",
      "Validation loss: 0.796938955783844 \n",
      "\n",
      "EPOCH 218\n",
      "Train loss: 0.7989315011284568\n",
      "Time needed for training: 19.595163345336914 seconds\n",
      "Validation loss: 0.7976400256156921 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "                {'params': model.backbone.parameters(), 'lr': 1e1},\n",
    "                {'params': model.decode.parameters(), 'lr': 1e-3}\n",
    "                ]\n",
    "                            ,amsgrad = True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 500)\n",
    "loss_fn_cross = Ai4MarsCrossEntropy().to(device)\n",
    "loss_fn_dice = Ai4MarsDiceLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = Ai4MarsTrainer(loss_fn_dice, optimizer, train_loader, val_loader, lr_scheduler, device = device)\n",
    "model.to(device)\n",
    "trainer.train_multiple_epoch(model, EPOCHS = 1400)\n",
    "# Plot the histogram\n",
    "trainer.param_hist(model)\n",
    "trainer.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "error",
     "timestamp": 1689164347160,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "FZzOcvNXLK19",
    "outputId": "6d586d17-2309-43bf-e0a8-02c5aa9036d4"
   },
   "outputs": [],
   "source": [
    "from semseg.datasets import *\n",
    "\n",
    "#model = model.to(device)\n",
    "predictions = []\n",
    "start = time.time()\n",
    "#print('test')\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move the data to the desired device\n",
    "        #print(images.shape)\n",
    "        #images = images.permute(0,3,1,2).to(device)\n",
    "        #images = images.permute(2,0,1).to(device)\n",
    "        #images = images [None, :, :, :]\n",
    "        #print(images.shape)\n",
    "\n",
    "        #print(images.shape)\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "        # Forward pass to get the predictions\n",
    "        with torch.inference_mode():\n",
    "          prediction = model(images)\n",
    "        #print(prediction)\n",
    "        prediction = prediction.softmax(1).argmax(1).to(int)\n",
    "        \n",
    "        #prediction = prediction.round().to(int)\n",
    "        #print(prediction.shape)\n",
    "        un = prediction.unique()\n",
    "        #print(un)\n",
    "        palette = eval('ADE20K').PALETTE.to(device)\n",
    "        prediction_map = palette[prediction].squeeze().to(torch.uint8)\n",
    "        #print(type(prediction_map))\n",
    "        #show_image(prediction_map)\n",
    "        predictions.append(prediction_map)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1689164347161,
     "user": {
      "displayName": "paolo renzi",
      "userId": "12765974689024670837"
     },
     "user_tz": -120
    },
    "id": "UKfyLsOXXBjx"
   },
   "outputs": [],
   "source": [
    "image, label = test_loader.dataset.__getitem__(10)\n",
    "\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "output = model(image)\n",
    "\n",
    "output = output.permute(0,2,1,3).permute(0,1,3,2)\n",
    "\n",
    "label = label.squeeze()\n",
    "label = label.long()\n",
    "\n",
    "print(loss_fn_dice(output, label))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sithu31296/semantic-segmentation\n",
    "%pip install -U gdown\n",
    "%pip install -e .\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt = Path('./checkpoints/pretrained/segformer')\n",
    "ckpt.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n",
    "output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5vQ0kHSCTjj",
    "outputId": "b0797876-7341-4b92-87ce-d94dd302072e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kytW-DGR_xaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/models/semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import io\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%cd semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from itertools import chain\n",
    "from math import ceil\n",
    "from base_model import BaseModel\n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "    def __init__(self, inchannels):\n",
    "        super(DecoderBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannels, inchannels//4, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inchannels//4)\n",
    "        self.conv2 = nn.ConvTranspose2d(inchannels//4, inchannels//4, kernel_size=2, stride=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inchannels//4)\n",
    "        self.conv3 = nn.Conv2d(inchannels//4, inchannels//2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(inchannels//2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(inchannels, inchannels//2, kernel_size=2, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(inchannels//2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class LastBottleneck(nn.Module):\n",
    "    def __init__(self, inchannels):\n",
    "        super(LastBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannels, inchannels//4, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inchannels//4)\n",
    "        self.conv2 = nn.Conv2d(inchannels//4, inchannels//4, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inchannels//4)\n",
    "        self.conv3 = nn.Conv2d(inchannels//4, inchannels//4, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(inchannels//4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inchannels, inchannels//4, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(inchannels//4))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paolo/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ResDecode(BaseModel):\n",
    "    def __init__(self, num_classes, pretrained=True, freeze_bn=False, **_):\n",
    "        super(ResDecode, self).__init__()\n",
    "        resnet50 = torchvision.models.resnet50()\n",
    "        num_classes = 10\n",
    "        # Decoder\n",
    "        resnet50_untrained = models.resnet50(pretrained=False)\n",
    "        resnet50_blocks = list(resnet50_untrained.children())[4:-2][::-1]\n",
    "        decoder = []\n",
    "        channels = (2048, 1024, 512)\n",
    "        for i, block in enumerate(resnet50_blocks[:-1]):\n",
    "            new_block = list(block.children())[::-1][:-1]\n",
    "            decoder.append(nn.Sequential(*new_block, DecoderBottleneck(channels[i])))\n",
    "        new_block = list(resnet50_blocks[-1].children())[::-1][:-1]\n",
    "        decoder.append(nn.Sequential(*new_block, LastBottleneck(256)))\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "        \n",
    "        self.last_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, bias=False),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        inputsize = x.size()\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        h_diff = ceil((x.size()[2] - indices.size()[2]) / 2)\n",
    "        w_diff = ceil((x.size()[3] - indices.size()[3]) / 2)\n",
    "        if indices.size()[2] % 2 == 1:\n",
    "            x = x[:, :, h_diff:x.size()[2]-(h_diff-1), w_diff: x.size()[3]-(w_diff-1)]\n",
    "        else:\n",
    "            x = x[:, :, h_diff:x.size()[2]-h_diff, w_diff: x.size()[3]-w_diff]\n",
    "\n",
    "        x = F.max_unpool2d(x, indices, kernel_size=2, stride=2)\n",
    "        x = self.last_conv(x)\n",
    "        \n",
    "        if inputsize != x.size():\n",
    "            h_diff = (x.size()[2] - inputsize[2]) // 2\n",
    "            w_diff = (x.size()[3] - inputsize[3]) // 2\n",
    "            x = x[:, :, h_diff:x.size()[2]-h_diff, w_diff: x.size()[3]-w_diff]\n",
    "            if h_diff % 2 != 0: x = x[:, :, :-1, :]\n",
    "            if w_diff % 2 != 0: x = x[:, :, :, :-1]\n",
    "\n",
    "        return x    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    module = nn.Module()\n",
    "    model = ResDecode(module)\n",
    "    #x = torch.zeros(1, 3, 224, 224)\n",
    "    #outs = model(x)\n",
    "    #for y in outs:\n",
    "    #    print(y.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nc):\n",
    "        nc = nc\n",
    "        super(Generator, self).__init__()\n",
    "       \n",
    "\n",
    "    def forward(self, embeds):\n",
    "        ngf = 64\n",
    "        nz = [[None] *6]*4\n",
    "        \n",
    "        j = 0\n",
    "        for embed in embeds:\n",
    "            nz[j][0] = embed.size()[1]\n",
    "            for i in range(1,6):\n",
    "                if i < 4:\n",
    "                    nz[j][i] = nz[i-1]//2\n",
    "                else:\n",
    "                    nz[j][i] = nz[i-1]//4\n",
    "            j = j+1\n",
    "            print(nz)\n",
    "        self.main = nn.Sequential(\n",
    "            #reduction of dimensionality To Be Changed in conv... maybe\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz[6], ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "        \n",
    "        return self.main(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Tuple\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)        # use SyncBN in original\n",
    "        self.activate = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.activate(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class SegFormerHead(nn.Module):\n",
    "    def __init__(self, dims: list, embed_dim: int = 256, num_classes: int = 19):\n",
    "        super().__init__()\n",
    "        for i, dim in enumerate(dims):\n",
    "            self.add_module(f\"linear_c{i+1}\", MLP(dim, embed_dim))\n",
    "\n",
    "        self.linear_fuse = ConvModule(embed_dim*4, embed_dim)\n",
    "        self.linear_pred = nn.Conv2d(embed_dim, num_classes, 1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, features: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        B, _, H, W = features[0].shape\n",
    "        outs = [self.linear_c1(features[0]).permute(0, 2, 1).reshape(B, -1, *features[0].shape[-2:])]\n",
    "\n",
    "        for i, feature in enumerate(features[1:]):\n",
    "            cf = eval(f\"self.linear_c{i+2}\")(feature).permute(0, 2, 1).reshape(B, -1, *feature.shape[-2:])\n",
    "            outs.append(F.interpolate(cf, size=(H, W), mode='bilinear', align_corners=False))\n",
    "\n",
    "        seg = self.linear_fuse(torch.cat(outs[::-1], dim=1))\n",
    "        seg = self.linear_pred(self.dropout(seg))\n",
    "        return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 1, 136, 136])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from semseg.models.base import BaseModel\n",
    "from semseg.models.heads import SegFormerHead\n",
    "#from resDecode import ResDecode\n",
    "\n",
    "\n",
    "class SegFormerpp(BaseModel):\n",
    "    def __init__(self, backbone: str = 'MiT-B0', num_classes: int = 19, head: str = 'B0') -> None:\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.head = head\n",
    "        self.decode = SegFormerHead(self.backbone.channels, 256 if 'B0' in backbone or 'B1' in backbone else 768, 3)\n",
    "        self.decode_head = Generator(num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.backbone(x)\n",
    "        #y = self.decode(y)\n",
    "        #y = F.interpolate(y, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        #embeds = []\n",
    "        #for h in y:\n",
    "        #    embed = torch.reshape(h,(h.shape[0],h.shape[1]*h.shape[2]*h.shape[3]))\n",
    "        #    embeds.append(embed)\n",
    "        y = self.decode(y)\n",
    "        print(y.shape)\n",
    "        upsample = nn.Sequential(\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],3,5),\n",
    "        torch.nn.ConvTranspose2d(y.shape[1],1,5)\n",
    "        )\n",
    "        y = upsample(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = SegFormerpp('MiT-B0')\n",
    "    # model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b0.ade.pth', map_location='cpu'))\n",
    "    x = torch.zeros(1, 3, 512, 512)\n",
    "    y = model(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bglipAaW_xaY",
    "outputId": "b0ce7f9f-af64-4898-fbba-803575c0985a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download a pretrained model's weights from the result table.\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "from semseg.models import *\n",
    "#from segFormerpp import SegFormerpp\n",
    "\n",
    "model = eval('SegFormerpp')(\n",
    "    backbone='MiT-B3',\n",
    "    num_classes=150\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('checkpointcheckpoints/pretrained/segformer/segformer.b3.ade.pth'))\n",
    "except:\n",
    "    print(\"Download a pretrained model's weights from the result table.\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AujMSfML_xaa",
    "outputId": "4531b764-b8ea-4f05-b569-9a908b6859ee"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/dataloader\n",
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/models/semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "%cd ../../dataloader/\n",
    "from load import Ai4MarsData\n",
    "%cd ../models/semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "\n",
    "#This class rappresents the dataset \n",
    "class Ai4MarsData(Dataset):\n",
    "    #X tensor (torch) -> images\n",
    "    #y tensor (torch) -> labels\n",
    "\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.X[index]\n",
    "        label = self.y[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    #this function return 3 dataloader (train,test,validation) splitted from self \n",
    "    #percentage -> give percentage of train size, the rest of percentage is given divided the residual part\n",
    "    #sizeBatch -> determine the size of batch\n",
    "    def splitLoader(self,percentage,sizeBatch):\n",
    "        dataset = self\n",
    "        ratio = percentage/100\n",
    "\n",
    "        #setup variables\n",
    "        d_size = len(self)\n",
    "        train_size = int(ratio*d_size)\n",
    "        test_size = int((d_size - train_size)/2)\n",
    "        validation_size = test_size\n",
    "\n",
    "        #split\n",
    "        train_dataset, test_dataset, validation_dataset = random_split(dataset,[train_size,test_size,validation_size])\n",
    "        \n",
    "        #create other loaders\n",
    "        train_loader = DataLoader(train_dataset,batch_size=sizeBatch)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=sizeBatch)\n",
    "        validation_loader = DataLoader(validation_dataset,batch_size=sizeBatch)\n",
    "        '''       \n",
    "        for images, labels in train_loader:\n",
    "            print(images.shape)\n",
    "            images = images.permute(0,3,1,2).to(device)\n",
    "      \n",
    "        for images, labels in test_loader:\n",
    "            images = images.permute(0,3,1,2).to(device)\n",
    "            \n",
    "        for images, labels in validation_loader:\n",
    "            images = images.permute(0,3,1,2).to(device)\n",
    "        '''\n",
    "            \n",
    "        return train_loader,test_loader,validation_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/airoprojects/visiope\n",
    "with open('/content/drive/MyDrive/Dataset/data_loader.pkl', 'rb') as f:\n",
    "    data_loader = pickle.load(f)\n",
    "\n",
    "\n",
    "items = data_loader['dataloader'].dataset.__getitem__(1)\n",
    "\n",
    "\n",
    "print(items[0].shape)\n",
    "\n",
    "\n",
    "plt.imshow(items[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "with open('../../dataloader/data_loader.pkl', 'rb') as f:\n",
    "    data_loader = pickle.load(f)\n",
    "\n",
    "\n",
    "items = data_loader['dataloader'].dataset.__getitem__(1)\n",
    "\n",
    "train_loader, test_loader, validation_loader = data_loader['dataloader'].dataset.splitLoader(80,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O3ytN-ugjHcW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  \\n  print(img.device)\\n  print(img.shape)\\n  if (img.device != 'cpu'):\\n    img.to('cpu')\\n    print('test')\\n  print(img.device)\\n  plt.imshow(img)\\n  plt.show()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show_image(imgs):\n",
    "    imgs = imgs.permute(2,0,1)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "'''  \n",
    "  print(img.device)\n",
    "  print(img.shape)\n",
    "  if (img.device != 'cpu'):\n",
    "    img.to('cpu')\n",
    "    print('test')\n",
    "  print(img.device)\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/loss\n",
      "/home/paolo/Downloads/magistrale/1 anno/visiope/visiope/models/semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "%cd ../../loss/\n",
    "from trainer_module import trainer\n",
    "%cd ../models/semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtraining_set\u001b[49m):\n\u001b[1;32m      2\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(training_set):\n",
    "        inputs, labels = data\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs.permute(0,3,1,2).to(device)\n",
    "        # Zero your gradients for every batch!+\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([1, 3, 56, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m validation_set \u001b[38;5;241m=\u001b[39m test_loader\n\u001b[1;32m      6\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m : model,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m : loss_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m : device\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/magistrale/1 anno/visiope/visiope/loss/trainer_module.py:18\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(parameters, multiple_epochs, epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     15\u001b[0m     train_multiple_epoch(parameters, EPOCHS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_writer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/magistrale/1 anno/visiope/visiope/loss/trainer_module.py:47\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(parameters, epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mSegFormerpp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m upsample \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConvTranspose2d(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConvTranspose2d(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "training_set = train_loader\n",
    "validation_set = test_loader\n",
    "\n",
    "parameters = {\n",
    "    'model' : model,\n",
    "    'loss' : loss_fn,\n",
    "    'optimizer' : optimizer,\n",
    "    'training' : training_set,\n",
    "    'validation' : validation_set,\n",
    "    'device' : device\n",
    "}\n",
    "trainer(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FZzOcvNXLK19",
    "outputId": "74ba437b-2425-499e-e311-5995b3673fee"
   },
   "outputs": [],
   "source": [
    "from semseg.datasets import *\n",
    "\n",
    "#model = model.to(device)\n",
    "predictions = []\n",
    "start = time.time()\n",
    "#print('test')\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move the data to the desired device\n",
    "        #print(images.shape)\n",
    "        images = images.permute(0,3,1,2).to(device)\n",
    "        #images = images.permute(2,0,1).to(device)\n",
    "        #images = images [None, :, :, :]\n",
    "        #print(images.shape)\n",
    "        \n",
    "        #print(images.shape)\n",
    "        #labels = labels.to(device)\n",
    "\n",
    "        # Forward pass to get the predictions\n",
    "        with torch.inference_mode():\n",
    "          prediction = model(images)\n",
    "        #print(prediction)\n",
    "        prediction = prediction.softmax(1).argmax(1).to(int)\n",
    "        #prediction = prediction.round().to(int)\n",
    "        #print(prediction.shape)\n",
    "        un = prediction.unique()\n",
    "        #print(un)\n",
    "        palette = eval('ADE20K').PALETTE.to(device)\n",
    "        prediction_map = palette[prediction].squeeze().to(torch.uint8)\n",
    "        #print(type(prediction_map))\n",
    "        show_image(prediction_map)\n",
    "        predictions.append(prediction_map)\n",
    "        \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

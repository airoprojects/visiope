{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/airoprojects/visiope/blob/main/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mpISdc8elhG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "qIXoYIV2wO0I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FCQvjMbelhL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sithu31296/semantic-segmentation\n",
        "%cd semantic-segmentation\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iiTAPAa9elhO",
        "outputId": "e0f03ca2-8e28-46a7-9ade-5bee8f7b8532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# On Colab the path to the module ti fixed once you have\n",
        "# corretly set up the project with gitsetup.ipynb\n",
        "fixed_path = '/content/drive/MyDrive/Github/visiope/dataloader/'\n",
        "sys.path.insert(0, fixed_path)\n",
        "\n",
        "# Insert here the path to the dataset on your drive\n",
        "data_path = '/content/drive/MyDrive/Dataset/'\n",
        "\n",
        "#to import trainer and loss\n",
        "fixed_path_loss = '/content/drive/MyDrive/Github/visiope/loss/'\n",
        "sys.path.insert(0, fixed_path_loss)\n",
        "\n",
        "fixed_path_trainer = '/content/drive/MyDrive/Github/visiope/trainer/'\n",
        "sys.path.insert(0,fixed_path_trainer )\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from asloader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n",
        "\n",
        "# sys.path.insert(1, '/content/drive/MyDrive/Github/visiope/loss')\n",
        "\n",
        "# import custom_loss"
      ],
      "metadata": {
        "id": "M62Uw4zVsMgK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlAwFGJelhQ"
      },
      "source": [
        "## Show Available Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dvk_2mt1elhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f17c242-666e-4aed-fe5e-42f0174f8b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  No.  Model Names\n",
            "-----  -------------\n",
            "    1  SegFormer\n",
            "    2  Lawin\n",
            "    3  SFNet\n",
            "    4  BiSeNetv1\n",
            "    5  DDRNet\n",
            "    6  FCHarDNet\n",
            "    7  BiSeNetv2\n"
          ]
        }
      ],
      "source": [
        "from semseg import show_models\n",
        "\n",
        "show_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPOfdSNgelhT"
      },
      "source": [
        "## Load a Pretrained Model\n",
        "\n",
        "Download a pretrained model's weights from the result table (ADE20K, CityScapes, ...) and put it in `checkpoints/pretrained/model_name/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sXtHvaHelhU"
      },
      "outputs": [],
      "source": [
        "%pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kx8AswxelhW"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "ckpt = Path('./checkpoints/pretrained/segformer')\n",
        "ckpt.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n",
        "output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZCa6tmRlelhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5973cbc1-d49d-4cdf-b9ff-4846dea2c8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download a pretrained model's weights from the result table.\n",
            "Loaded Model\n"
          ]
        }
      ],
      "source": [
        "from semseg.models import *\n",
        "\n",
        "model = eval('SegFormer')(\n",
        "    backbone='MiT-B3',\n",
        "    num_classes=150\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth', map_location='cpu'))\n",
        "except:\n",
        "    print(\"Download a pretrained model's weights from the result table.\")\n",
        "model.eval()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print('Loaded Model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Dataset/dataset.zip -d /content/drive/MyDrive/Dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVgYCY-TvpGu",
        "outputId": "64970288-a7c8-4fba-8e04-4721048e2a94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Dataset/dataset.zip\n",
            "replace /content/drive/MyDrive/Dataset/dataset/val.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/train.pt\")\n",
        "val_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/val.pt\")\n",
        "test_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/test.pt\")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "#COLPA MIAA (MY FAULT)!!!!!!!\n",
        "train_set.setDevice(device)\n",
        "val_set.setDevice(device)\n",
        "test_set.setDevice(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_set.conversion('f')\n",
        "test_set.conversion('f')\n",
        "val_set.conversion('f')\n",
        "\n",
        "train_set.set_grad()\n",
        "test_set.set_grad()\n",
        "val_set.set_grad()\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "jmRVji71wGJc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#control to delete\n",
        "print(len(train_set))\n",
        "\n",
        "image, label = train_loader.dataset.__getitem__(0)\n",
        "print(image.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_1tgi891tWo",
        "outputId": "82e626cb-ab8d-4a11-9a49-38d5d89f32fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmy96qA8elhY"
      },
      "source": [
        "## Simple Image Inference\n",
        "\n",
        "### Load Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4rwmyO_elhb"
      },
      "source": [
        "### Model Forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4iG9U9kelhc"
      },
      "source": [
        "### Postprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNc5hKBPelhj"
      },
      "source": [
        "### Choose a Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sSxW3mFNelhl"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "#START TRAIN\n",
        "# import train and loss\n",
        "from astrainer import Ai4MarsTrainer\n",
        "import asloss\n",
        "\n",
        "loss_fn =  torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "training_set = train_loader\n",
        "test_set =  test_loader\n",
        "trainer = Ai4MarsTrainer(loss_fn, optimizer, training_set, test_set)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import NONE\n",
        "image = 0\n",
        "label = 0\n",
        "for images, labels in train_loader:\n",
        "  image = images\n",
        "  label = labels\n",
        "  break\n",
        "\n",
        "\n",
        "prediction = model(image)\n",
        "print(label.type())\n",
        "label = label.type(torch.FloatTensor)\n",
        "print(label.type())\n",
        "\n",
        "\n",
        "print(prediction.shape)\n",
        "new_prediction = torch.argmax(prediction, dim=1)\n",
        "print(f\"prediction BEFORE shape -> {new_prediction.shape}\")\n",
        "\n",
        "new_prediction = new_prediction[:,None, :, :]\n",
        "print(f\"prediction AFTER shape -> {new_prediction.shape}\")\n",
        "print(f\"label shape -> {label.shape}\")\n",
        "\n",
        "print(new_prediction.type())\n",
        "new_prediction = new_prediction.type(torch.FloatTensor)\n",
        "print(label.type())\n",
        "\n",
        "print(loss_fn(new_prediction, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYTig4Zx8Uhi",
        "outputId": "20b246ff-0cd6-494b-8b1b-3a226abda391"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.FloatTensor\n",
            "torch.FloatTensor\n",
            "torch.Size([64, 150, 64, 64])\n",
            "prediction BEFORE shape -> torch.Size([64, 64, 64])\n",
            "prediction AFTER shape -> torch.Size([64, 1, 64, 64])\n",
            "label shape -> torch.Size([64, 1, 64, 64])\n",
            "torch.cuda.LongTensor\n",
            "torch.FloatTensor\n",
            "tensor(-0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_multiple_epoch(model,EPOCHS=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "FSkhk_Rl2gX0",
        "outputId": "6ad26dc4-7bee-4105-8407-f086372c1f6f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d7ac8cb80abc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_multiple_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Github/visiope/trainer/astrainer.py\u001b[0m in \u001b[0;36mtrain_multiple_epoch\u001b[0;34m(self, model, EPOCHS)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Make sure gradient tracking is on, and do a pass over the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# We don't need gradients on to do reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Github/visiope/trainer/astrainer.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, model, epoch_index)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Adjust learning weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
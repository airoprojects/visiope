{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9FCQvjMbelhL"},"outputs":[],"source":["# Set up existing models\n","!git clone https://github.com/sithu31296/semantic-segmentation\n","%cd semantic-segmentation\n","%pip install -e .\n","%pip install -U gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiTAPAa9elhO"},"outputs":[],"source":["# Custom imports: Dataset, Preprocessor, Dataloader, Training and Loss functions\n","\n","import sys\n","import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n","\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if not IN_COLAB:\n","\n","    from git import Repo\n","\n","    # Initialize the Git repository object\n","    repo = Repo(\".\", search_parent_directories=True)\n","\n","    # Get the root directory of the Git project\n","    root_dir = repo.git.rev_parse(\"--show-toplevel\")\n","\n","    from pathlib import Path\n","\n","    # Set up path for custom importer modules\n","    # Data Loader\n","    importer_module = root_dir + '/dataloader/'\n","    sys.path.insert(0, importer_module)\n","    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n","\n","    # Loss\n","    loss_module = root_dir + '/trainer/loss/'\n","    sys.path.insert(0, loss_module)\n","    import loss\n","\n","    # Trainer\n","    trainer_module = root_dir + '/trainer/'\n","    sys.path.insert(0, trainer_module)\n","    from trainer import Ai4MarsTrainer\n","\n","    # Insert here your local path to the dataset (temporary)\n","    data_path = '/home/leeoos/Desktop/'\n","\n","else: # IN_COLAB\n","\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # On Colab the path to the module ti fixed once you have\n","    # corretly set up the project with gitsetup.ipynb\n","\n","    # Import Loader\n","    fixed_path_loader = '/content/drive/MyDrive/Github/visiope/dataloader/'\n","    sys.path.insert(0, fixed_path_loader)\n","    from loader import Ai4MarsImporter, Ai4MarsProcessor, Ai4MarsData\n","\n","    # Import Trainer\n","    fixed_path_trainer = '/content/drive/MyDrive/Github/visiope/trainer/'\n","    sys.path.insert(0,fixed_path_trainer )\n","    from trainer import Ai4MarsTrainer\n","\n","    # Import Loss\n","    fixed_path_loss = '/content/drive/MyDrive/Github/visiope/trainer/loss/'\n","    sys.path.insert(0, fixed_path_loss)\n","    import loss\n","\n","    # Get Dataset as np array\n","    np_dataset = True\n","\n","    # Get Dataset from original sources (slower)\n","    original_dataset = False\n","\n","    if original_dataset and not np_dataset:\n","        # Insert here the path to the dataset on your drive\n","        data_path = input(\"Path to Dataset: \") #'/content/drive/MyDrive/foo/'\n","\n","    else:\n","        import gdown\n","\n","        # get url of np dataset (temporarerly my drive)\n","        url_X = 'https://drive.google.com/uc?id=1HLAQgjZbGa3lMzdyIvgykCiUJ6P4OaEX'\n","        url_y = 'https://drive.google.com/uc?id=1Ue74LEe0WlEnFxRcIl19WyvXR1EyYIsS'\n","\n","        # download np dataset on runtime env\n","        data_path = '/content/dataset/'\n","        gdown.download(url_X, data_path, quiet=False)\n","        gdown.download(url_y, data_path, quiet=False)\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jmRVji71wGJc","executionInfo":{"status":"ok","timestamp":1688660395406,"user_tz":-120,"elapsed":8937,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[],"source":["from IPython.testing import test\n","# Build up Ai4MarsDataset\n","\n","if not IN_COLAB:\n","\n","    data_import = Ai4MarsImporter()\n","\n","    X, y = data_import(path=data_path, num_of_images=200)\n","\n","    processor = Ai4MarsProcessor(X, y)\n","    train_set, test_set, val_set = processor([0.54, 0.26, 0.20])\n","\n","else:\n","    X = np.load('/content/dataset/X.npy')\n","    y = np.load('/content/dataset/y.npy')\n","\n","    processor = Ai4MarsProcessor()\n","    train_set, test_set, val_set = processor(X, y, [0.54, 0.26, 0.20])\n","\n","\n","    # train_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/train.pt\")\n","    # val_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/val.pt\")\n","    # test_set = torch.load(\"/content/drive/MyDrive/Dataset/dataset/test.pt\")\n","\n","\n","\n","#COLPA MIAA (MY FAULT)!!!!!!!\n","# train_set.setDevice(device)\n","# val_set.setDevice(device)\n","# test_set.setDevice(device)\n","\n","train_set.resize(64)\n","test_set.resize(64)\n","\n","train_set.conversion('f')\n","test_set.conversion('f')\n","val_set.conversion('f')\n","\n","train_set.set_grad()\n","test_set.set_grad()\n","val_set.set_grad()\n","\n","train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=5, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=5, shuffle=True)"]},{"cell_type":"code","source":["print(len(train_set))\n","print(len(train_loader))\n","print(train_loader.dataset.__getitem__(0)[0].shape)"],"metadata":{"id":"sZQgsFxWbl_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvk_2mt1elhR"},"outputs":[],"source":["from semseg import show_models\n","\n","show_models()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-kx8AswxelhW","executionInfo":{"status":"ok","timestamp":1688660395770,"user_tz":-120,"elapsed":5,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[],"source":["allow_download = False\n","\n","if allow_download:\n","    import gdown\n","    from pathlib import Path\n","\n","    ckpt = Path('./checkpoints/pretrained/segformer')\n","    ckpt.mkdir(exist_ok=True, parents=True)\n","\n","    url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n","    output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n","\n","    gdown.download(url, output, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCa6tmRlelhW"},"outputs":[],"source":["#  Import segformer\n","\n","from semseg.models import *\n","\n","model = eval('SegFormer')(\n","    backbone='MiT-B0',\n","    num_classes=5\n",")\n","\n","try:\n","    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth', map_location='cpu'))\n","except:\n","    print(\"Download a pretrained model's weights from the result table.\")\n","model.eval()\n","\n","model.to(device)\n","\n","print('Loaded Model')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"FVgYCY-TvpGu","executionInfo":{"status":"ok","timestamp":1688660401821,"user_tz":-120,"elapsed":32,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[],"source":["#!unzip /content/drive/MyDrive/Dataset/dataset.zip -d /content/drive/MyDrive/Dataset/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_1tgi891tWo"},"outputs":[],"source":["#control to delete\n","print(len(train_set))\n","\n","image, label = train_loader.dataset.__getitem__(0)\n","print(image.requires_grad)"]},{"cell_type":"code","source":["import sys\n","import torch\n","from pathlib import Path\n","from datetime import datetime\n","\n","# This class collects all the training functionalities to train different models\n","class TmpTrainer():\n","\n","    # Initialization of training parameters in the class constructor\n","    def __init__(self, loss_fn, optimizer, train_loader, test_loader, device):\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","        self.device = device\n","\n","    # This function implements training for just one epoch\n","    def train_one_epoch(self, model, epoch_index=0):\n","        accumulated_loss = 0.\n","        last_loss = 0.\n","\n","        for batch_index, batch in enumerate(self.train_loader):\n","            # Every data instance is an (input, label) pair\n","            inputs, labels = batch\n","\n","            # Zero your gradients for every batch!\n","            self.optimizer.zero_grad()\n","\n","            # Send inputs and labels to GPU\n","            inputs = inputs.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            # Make predictions for this batch\n","            outputs = model(inputs)\n","\n","            # print(outputs.requires_grad)\n","            # print(f'output shape: {outputs.shape}')\n","            # print(f'labels shape: {labels.shape}')\n","\n","            # TEMPORARY MODIFICATIONS\n","            #NEED TO MODIFY FOR EACH MODEL, BASED ON LABEL!!!!!!!!!!!!\n","            # new_pred = torch.argmax(outputs, dim=1)\n","            new_pred = torch.max(outputs, dim=1, keepdim=True)[0]\n","            # print(new_pred.requires_grad)\n","            # print(f'arg max shape: {new_pred.shape}')\n","\n","            #new_pred = new_pred[None, :, :, :] FOR MODEL -> UNKNOWN\n","            # new_pred = new_pred.unsqueeze(1)\n","            # new_pred = new_pred[:, None, :, :]\n","            # print(new_pred.requires_grad)\n","            # new_pred = new_pred.type(torch.float32)\n","            # print(f'squeeze shape: {new_pred.shape}')\n","            # END OF TEMPORARY MODIFICATIONS\n","\n","            # print(new_pred.requires_grad)\n","            #new_pred.requires_grad = True\n","\n","            # Compute the loss and its gradients\n","            loss = self.loss_fn(new_pred, labels)\n","            loss.backward()\n","\n","            # Adjust learning weights\n","            self.optimizer.step()\n","\n","            accumulated_loss += loss.item()\n","            #report_index = len(self.train_loader) -1\n","\n","            # Free VRAM\n","            inputs.detach()\n","            labels.detach()\n","            del inputs\n","            del labels\n","\n","        # Compute the average loss over all batches\n","        last_loss =  accumulated_loss / batch_index+1\n","\n","        # Print report at the end of the last batch\n","        print(f'Epoch {epoch_index+1} loss: {last_loss}')\n","\n","        return last_loss\n","\n","    # This function implements training for multiple epochs\n","    def train_multiple_epoch(self, model, EPOCHS=100):\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        epoch_number = 0 # just a counter\n","        best_tloss = 1_000_000.\n","\n","        for epoch in range(EPOCHS):\n","            print(f'EPOCH: {epoch+1}')\n","\n","            # Make sure gradient tracking is on, and do a pass over the data\n","            model.train(True)\n","            avg_loss = self.train_one_epoch(model, epoch)\n","\n","            # We don't need gradients on to do reporting\n","            # model.train(False)\n","\n","            # Test loss\n","            accumulated_tloss = 0.0\n","            for tbatch_index, tbatch in enumerate(self.test_loader):\n","\n","                # Every data instance is a (input, label) pair\n","                tinputs, tlabels = tbatch\n","\n","                # Send inputs and labels to GPU\n","                tinputs = tinputs.to(self.device)\n","                tlabels = tlabels.to(self.device)\n","\n","                # Model prediction\n","                toutputs = model(tinputs)\n","\n","                toutputs = torch.max(toutputs, dim=1, keepdim=True)[0]\n","\n","                # Run test loss\n","                test_loss = self.loss_fn(toutputs, tlabels)\n","                accumulated_tloss += test_loss.item()\n","\n","                tinputs.detach()\n","                tlabels.detach()\n","                del tinputs\n","                del tlabels\n","\n","            # Compute the average loss over all batches\n","            avg_tloss = accumulated_tloss / (tbatch_index + 1)\n","\n","            # Print report at the end of the epoch\n","            print(f'LOSS train {avg_loss} test {avg_tloss}')\n","\n","            # Track best performance, and save the model's state\n","            if avg_tloss < best_tloss:\n","                best_tloss = avg_tloss\n","                model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","                torch.save(model.state_dict(), model_path)\n","\n","if __name__ == '__main__':\n","    pass"],"metadata":{"id":"8adJT2MU180a","executionInfo":{"status":"ok","timestamp":1688660401822,"user_tz":-120,"elapsed":30,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"sSxW3mFNelhl","executionInfo":{"status":"ok","timestamp":1688660401823,"user_tz":-120,"elapsed":30,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[],"source":["#START TRAIN\n","\n","loss_fn =  torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","training_set = train_loader\n","test_set =  test_loader\n","\n","trainer = TmpTrainer(loss_fn, optimizer, training_set, test_set, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSkhk_Rl2gX0"},"outputs":[],"source":["trainer.train_multiple_epoch(model,EPOCHS=10)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":553},"id":"VYTig4Zx8Uhi","outputId":"58ba51ad-d280-4706-d3a7-6dcb64c0ed79","executionInfo":{"status":"error","timestamp":1688662927799,"user_tz":-120,"elapsed":399,"user":{"displayName":"Leonardo Colosi","userId":"10241973221821838785"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["image shape: torch.Size([64, 3, 64, 64])\n","pred shape: torch.Size([64, 5, 64, 64])\n","label shape = torch.Size([64, 1, 64, 64])\n","\n","pred shape torch.Size([64, 64, 64, 5])\n","label shape torch.Size([64, 64, 64, 1])\n","\n","pred shape torch.Size([64, 64, 5])\n","label shape torch.Size([64, 64, 1])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-c0105f54918b>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# print(f'pred shape {new_pred[0].shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# print(prediction.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of size: : [64, 64, 64, 1]"]}],"source":["\n","image, label = next(iter(train_loader))\n","\n","print(f'image shape: {image.shape}')\n","\n","image = image.to(device)\n","label = label.to(device)\n","\n","prediction = model(image)\n","print(f'pred shape: {prediction.shape}')\n","# print(label.type())\n","label = label.type(torch.FloatTensor)\n","# print(label.type())\n","print(f'label shape = {label.shape}')\n","\n","s = torch.nn.Softmax(dim=1)\n","new_pred = prediction.permute(0,2,1,3).permute(0,1,3,2)\n","label = label.permute(0,2,1,3).permute(0,1,3,2)\n","\n","print()\n","print(f'pred shape {new_pred.shape}')\n","print(f'label shape {label.shape}')\n","\n","# print(new_pred[0].argmax(dim=-1))\n","# print(label.squeeze(dim=-1)[0])\n","\n","print()\n","# new_pred = new_pred.max(dim=-1)[0]\n","new_pred = new_pred.to(device)\n","label = label.to(device)\n","\n","print(f'pred shape {new_pred[0].shape}')\n","print(f'label shape {label[0].shape}')\n","\n","# print(f'pred shape {new_pred[0].shape}')\n","\n","print(loss_fn(new_pred, label))\n","\n","# print(prediction.shape)\n","# new_prediction = torch.argmax(prediction, dim=1)\n","# print(f\"prediction BEFORE shape -> {new_prediction.shape}\")\n","\n","# new_prediction = new_prediction[:,None, :, :]\n","# print(f\"prediction AFTER shape -> {new_prediction.shape}\")\n","# print(f\"label shape -> {label.shape}\")\n","\n","# print(new_prediction.type())\n","# new_prediction = new_prediction.type(torch.FloatTensor)\n","# print(label.type())\n","\n","# print(loss_fn(new_prediction, label))\n","\n","# image.detach()\n","# label.detach()\n","# del label\n","# del image\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}